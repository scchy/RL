# 基于价值的传统表格方法总结

## 5.1 表格方法总结

### 5.1.1 monta carlo

**迭代时间**：一个回合结束后, 对每个动作进行迭代  
**回合内行动策略**：探索+利用  
**迭代公式**：
<font color=darkred>**当前状态行动带来的收益$Q_t(s, a)$加一定比例的行动增益(行动收益$G_t$减去当前收益)**</font>:  $Q_{t+1}(s, a)=Q_t(s, a) + \alpha * (G_t - Q_t(s, a) )$

- 行动收益$G_t$等于后续所有动作的奖励乘折扣系数: $G_t = \sum_{t}^{T}\gamma ^ t * R_t$

**脚本实现**：

```python
s = env.reset()
done = False
# 进行游戏，直到回合结束
experience = []
while not done:
    if render:
        env.render()
    a = self.policy(s, actions)
    n_state, reward, done, info = env.step(a)
    experience.append({"state": s, "action": a, "reward": reward})
    s = n_state
else:
    self.log(reward)

# 估计各种状态、行动
for i, x in enumerate(experience):
    s, a = x["state"], x["action"]

    # 计算状态s对应的折现值
    G, t = 0, 0
    for j in range(i, len(experience)):
        G += math.pow(gamma, t) * experience[j]["reward"]
        t += 1

    N[s][a] += 1  # s, a 对的数量
    alpha = 1 / N[s][a]
    self.Q[s][a] += alpha * (G - self.Q[s][a])
```

<font color=darkred>monte calo每次都是针对行动轨迹进行迭代学习，为同策略学习。反应会相对较慢，相对保守。</font>

### 5.1.2 Qlearning

**迭代时间**：每个动作之后就进行迭代  
**回合内行动策略**：探索+利用  
**迭代公式**：
<font color=darkred>**当前状态行动带来的收益$Q_t(s, a)$加一定比例的策略增益(策略收益$G_t$减去当前收益)**</font>:  $Q_{t+1}(s, a)=Q_t(s, a) + \alpha * (G_t - Q_t(s, a) )$

- 策略收益$G_t$等于当前行动奖励加折扣系数乘以下一状态最大行动价值：$G_t = R_t(s, a) + \gamma * max_a(Q_{t+1}(s'))$


**脚本实现**:

```python
s = env.reset()
done = False
while not done:
    if render:
        env.render()
    a = self.policy(s, actions)
    n_state, reward, done, info = env.step(a)

    gain = reward + gamma * max(self.Q[n_state])
    estimated = self.Q[s][a]
    self.Q[s][a] += learning_rate * (gain - estimated)
    s = n_state

else:
    self.log(reward)
```

<font color=darkred>Q学习每次都用改状态下个最大奖励动作来进行更新，是异策略。表现的会更加的莽撞、激进。</font>

### 5.1.3 SARSA

**迭代时间**：每个动作之后就进行迭代  
**回合内行动策略**：探索+利用  
**迭代公式**：
<font color=darkred>**当前状态行动带来的收益$Q_t(s, a)$加一定比例的行动增益(行动收益$G_t$减去当前收益)**</font>:  $Q_{t+1}(s, a)=Q_t(s, a) + \alpha * (G_t - Q_t(s, a) )$

- 行动收益$G_t$等于当前行动奖励加折扣系数乘以下一状态行动价值: $G_t = R_t(s, a) + \gamma * Q_{t+1}(s', a')$

<font color=darkred>和Qlearn的唯一区别就是学习的是，是同策略的迭代学习</font>

#### 多步SARSA

**迭代时间**：每个动作之后`n步`就进行迭代  
**回合内行动策略**：探索+利用  
**迭代公式**：
<font color=darkred>**当前状态行动带来的收益$Q_t(s, a)$加一定比例的行动增益(行动收益$G_t$减去当前收益)**</font>:  $Q_{t+1}(s, a)=Q_t(s, a) + \alpha * (G_t - Q_t(s, a) )$

- 行动收益$G_t$等于当前行动奖励加折扣系数乘以下一状态行动价值: $G_t = R_t(s, a) + \gamma * Q_{t+1}(s_{t+1}, a_{t+1}) + \gamma^2 * Q_{t+2}(s_{t+2}, a_{t+2}) + ... + Q_{t+n}(s_{t+n}, a_{t+n})  $

简单迭代实现

```python
 if len(self.state_list) == self.n:  # 若保存的数据可以进行n步更新
     G = self.Q_table[s1, a1]  # 得到Q(s_{t+n}, a_{t+n})
     for i in reversed(range(self.n)):
         G = self.gamma * G + self.reward_list[i]  # 不断向前计算每一步的回报
         # 如果到达终止状态,最后几步虽然长度不够n步,也将其进行更新
         if done and i > 0:
             s = self.state_list[i]
             a = self.action_list[i]
             self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])
     # n步Sarsa的主要更新步骤
     self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])
```

### 5.1.4 ActorCritic

**迭代时间**：每个动作之后就进行迭代  
**回合内行动策略**：探索+利用  
**特殊构造**：Actor负责探索($Q_t(s, a)$), Critic负责评估状态价值($V_t(s)$)。
**迭代公式**： 
<font color=darkred>**当前状态行动带来的收益$Q_t(s, a)$加一定比例的策略增益$td_t$**</font>: $Q_{t+1}(s, a)=Q_t(s, a) + \alpha * td_t$

- 策略增益$td_t$等于当前行动奖励($R_t$)加上一定比例($\gamma$)的下一状态评估价值($V_{t+1}(s_{t+1})$)减去当前状态评估价值($V_t(s)$)

$$Actor: Q_{t+1}(s, a)=Q_t(s, a) + \alpha * td_t$$
$$Critic: V_t(s) = V_t(s) + \alpha * td_t$$
$$td_t = R_t + \gamma * V_{t+1}(s_{t+1}) - V_t(s)$$

**脚本实现**:

```python
s = env.reset()
done = False
while not done:
    if render:
        env.render()
    a = actor.policy(s)
    n_state, reward, done, info = env.step(a)

    gain = reward + gamma * critic.V[n_state]
    estimated = critic.V[s]
    td = gain - estimated
    actor.Q[s][a] += learning_rate * td
    critic.V[s] += learning_rate * td
    s = n_state

else:
    actor.log(reward)

```

## 5.2 总结

|方法|迭代公式|$G_t$收益计算 |
|-|-|-|
|Montana carlo|$Q_t(s, a)=Q_t(s, a)+\alpha(G_t - Q_t(s, a))$| $G_t=\sum_t^T\gamma^tR_t$|
|Q-learnig|$Q_t(s, a)=Q_t(s, a)+\alpha(G_t - Q_t(s, a))$|$G_t=R_t + \gamma max_a(Q_{t}(s_{t+1}))$|
|SARSA|$Q_t(s, a)=Q_t(s, a)+\alpha(G_t - Q_t(s, a))$|$G_t=R_t + \gamma Q_{t}(s_{t+1}, a_{t+1})$|
|Actor-Critic|$A_t(s, a)=A_t(s, a)+\alpha (G_t - V_t(s)); V_t(s) = V_t(s) + \alpha (G_t - V_t(s))$|$G_t=R_t + \gamma V_{t}(s_{t+1})$|

## 5.3 DataWhale习题回答

### 构成强化学习的马尔可夫决策过程（MDP）的四元组有哪些变量？

状态(`state`)、动作(`action`)、状态转移概率(`transition prob`)、奖励(`reward`)

### 请通俗化描述强化学习的“学习”流程

针对一项序列优化的任务

1. 罗列实现这个任务的全部路径（或者足够多次的执行路径）
2. 计算不同路径的得分（平衡即时奖励与未来奖励）
   1. 计算一个路径上每个状态（节点）的价值（行动与奖励乘积）
   2. 行动等于行动概率$\pi(a|s)$ 乘以 状态转移概率$T(s'|s, a)$
   3. 奖励采用折扣率，将即时奖励与未来奖励加和，即$G_{t}=r_{t+1} + \gamma G_{t+1}$
3. 选择最优的路径。

### 请描述基于Sarsa 算法的智能体的学习过程

Sarsa是直接估计Q表格，得到Q表格，就可以更新策略（直接用智能体的下一个状态作为预估值）：

1. 用被训练的Q表格，对应环境反馈状态和奖励选取对应的动作进行输出。
2. 基于$(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$这几个值，并直接使用$A_{t+1}$去更新Q表格

### Q-learning算法和Sarsa算法的区别？

- Sarsa优化的是同策略学习
  - 直接拿下一步会执行的动作来优化Q表格
  - 只存在一种策略，它用一种策略去做动作的选取，也只用一种策略去优化。
- Q学习有两种不同的策略：目标策略(`policy(state)`)和行为策略（`max(Q[n_state])`）
  - 目标策略：
    - 需要去学习的策略
    - 可以根据自己的经验来学习最优的策略，不需要去和环境交互
  - 行为策略(探索环境的策略)：
    - `u`可以大胆地去探索到所有可能的轨迹，采集轨迹，采集数据，把采集到的数据喂给目标策略去学习
  - 目标函数优化
    - Q不会管你下一步去往哪里，它就只选收益最大的策略
    - 同时异策略可以让我们学习其他智能体的行为，模仿学习，学习人或者其他智能体产生的轨迹
    - $Q(s) = Q(s) + \alpha * (G_t - Q(s)); G_t = r + \gamma * max(Q[n\_state])$

### 同策略（on-policy）和异策略（off-policy）的区别

基本同上。

- 异策略算法是非常激进的，希望每一步都获得最大的利益；
- 同策略算法则相对来说偏保守，会选择一条相对安全的迭代路线。
