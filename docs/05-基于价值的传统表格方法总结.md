# 基于价值的传统表格方法总结

## 5.1 DataWhale习题回答

### 构成强化学习的马尔可夫决策过程（MDP）的四元组有哪些变量？

状态(`state`)、动作(`action`)、状态转移概率(`transition prob`)、奖励(`reward`)

### 请通俗化描述强化学习的“学习”流程

针对一项序列优化的任务

1. 罗列实现这个任务的全部路径（或者足够多次的执行路径）
2. 计算不同路径的得分（平衡即时奖励与未来奖励）
   1. 计算一个路径上每个状态（节点）的价值（行动与奖励乘积）
   2. 行动等于行动概率$\pi(a|s)$ 乘以 状态转移概率$T(s'|s, a)$
   3. 奖励采用折扣率，将即时奖励与未来奖励加和，即$G_{t}=r_{t+1} + \gamma G_{t+1}$
3. 选择最优的路径。

### 请描述基于Sarsa 算法的智能体的学习过程

Sarsa是直接估计Q表格，得到Q表格，就可以更新策略（直接用智能体的下一个状态作为预估值）：
1- 用被训练的Q表格，对应环境反馈状态和奖励选取对应的动作进行输出。
2- 基于$(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$这几个值，并直接使用$A_{t+1}$去更新Q表格

### Q-learning算法和Sarsa算法的区别？

- Sarsa优化的是同策略学习
  - 直接拿下一步会执行的动作来优化Q表格
  - 只存在一种策略，它用一种策略去做动作的选取，也只用一种策略去优化。
- Q学习有两种不同的策略：目标策略(`policy(state)`)和行为策略（`max(Q[n_state])`）
  - 目标策略：
    - 需要去学习的策略
    - 可以根据自己的经验来学习最优的策略，不需要去和环境交互
  - 行为策略(探索环境的策略)：
    - `u`可以大胆地去探索到所有可能的轨迹，采集轨迹，采集数据，把采集到的数据喂给目标策略去学习
  - 目标函数优化
    - Q不会管你下一步去往哪里，它就只选收益最大的策略
    - 同时异策略可以让我们学习其他智能体的行为，模仿学习，学习人或者其他智能体产生的轨迹
    - $Q(s) = Q(s) + \alpha * (G_t - Q(s)); G_t = r + \gamma * max(Q[n\_state])$

### 同策略（on-policy）和异策略（off-policy）的区别

基本同上。

- 异策略算法是非常激进的，希望每一步都获得最大的利益；
- 同策略算法则相对来说偏保守，会选择一条相对安全的迭代路线。

## 5.2 表格方法总结

### 5.2.1 monta carlo

**迭代时间**：一个回合结束后, 对每个动作进行迭代  
**回合内行动策略**：探索+利用  
**迭代公式**：
<font color=darkred>**当前状态行动带来的收益($Q_{(<s, a>, t)}$)加一定比例的行动收益($G_t$)</font>;行动收益等于后续所有动作的奖励乘折扣系数($\gamma ^ t$)之和减去当前状态行动带来的收益($Q_{(<s, a>, t)}$)**
$$Q_{(<s, a>, t+1)}=Q_{(<s, a>, t)} + \alpha * (G_t - Q_{(<s, a>, t)} )\\ \\ 
G_t = \sum_{t}^{T}\gamma ^ t * R_t$$
**脚本实现**：

```python
s = env.reset()
done = False
# 进行游戏，直到回合结束
experience = []
while not done:
    if render:
        env.render()
    a = self.policy(s, actions)
    n_state, reward, done, info = env.step(a)
    experience.append({"state": s, "action": a, "reward": reward})
    s = n_state
else:
    self.log(reward)

# 估计各种状态、行动
for i, x in enumerate(experience):
    s, a = x["state"], x["action"]

    # 计算状态s对应的折现值
    G, t = 0, 0
    for j in range(i, len(experience)):
        G += math.pow(gamma, t) * experience[j]["reward"]
        t += 1

    N[s][a] += 1  # s, a 对的数量
    alpha = 1 / N[s][a]
    self.Q[s][a] += alpha * (G - self.Q[s][a])
```

monte calo每次都是针对行动轨迹进行迭代学习，为同策略学习。反应会相对较慢，相对保守。

### 5.2.2 Qlearning

**迭代时间**：每个动作之后就进行迭代  
**回合内行动策略**：探索+利用  
**迭代公式**：<font color=darkred>**当前状态行动带来的收益($Q_{(<s, a>, t)}$)加一定比例的行动收益($G_t$)**</font>;  
行动收益等于当前行动奖励($R_{(<s, a>, t)}$)加折扣系数($\gamma$)乘以下一状态最大行动价值($max(Q_{(<s>, t+1)})$)减去**当前状态行动带来的收益($Q_{(<s, a>, t)}$)**
$$Q_{(<s, a>, t)} = Q_{(<s, a>, t)} + \alpha * (G_t -  Q_{(<s, a>, t)} )\\ \\
G_t = R_{(<s, a>, t)} + \gamma * max(Q_{(<s>, t+1)})$$

**脚本实现**:

```python
s = env.reset()
done = False
while not done:
    if render:
        env.render()
    a = self.policy(s, actions)
    n_state, reward, done, info = env.step(a)

    gain = reward + gamma * max(self.Q[n_state])
    estimated = self.Q[s][a]
    self.Q[s][a] += learning_rate * (gain - estimated)
    s = n_state

else:
    self.log(reward)
```
Q学习每次都用改状态下个最大奖励动作(不一定是当前动作)来进行更新，是异策略。表现的会更加的莽撞、激进。

### 5.2.3 SARSA

**迭代时间**：每个动作之后就进行迭代  
**回合内行动策略**：探索+利用  
**迭代公式**：<font color=darkred>**当前状态行动带来的收益($Q_{(<s, a>, t)}$)加一定比例的行动收益($G_t$)**</font>;  
行动收益等于当前行动奖励($R_{(<s, a>, t)}$)加折扣系数($\gamma$)乘以下一状态行动价值($max(Q_{(<s>, t+1)})$)减去**当前状态行动带来的收益($Q_{(<s, a>, t)}$)**

$$Q_{(<s, a>, t)} = Q_{(<s, a>, t)} + \alpha * (G_t -  Q_{(<s, a>, t)} )\\ 
G_t = R_{(<s, a>, t)} + \gamma * Q_{(<s>, t+1)}$$
<font color=darkred>和Qlearn的唯一区别就是学习的是下一状态的行动价值，是同策略的迭代学习</font>

#### 多步SARSA

**迭代时间**：每个动作之后`n步`就进行迭代  
**回合内行动策略**：探索+利用  
**迭代公式**：<font color=darkred>**当前状态行动带来的收益($Q_{(<s, a>, t)}$)加一定比例的n步行动收益($G_t$)**</font>;  
$$Q_{(<s, a>, t)} = Q_{(<s, a>, t)} + \alpha * (G_t -  Q_{(<s, a>, t)} )\\ 
G_t = R_{(<s, a>, t)} + \gamma *  R_{(<s, a>, t+1)}  + \gamma^2 *  R_{(<s, a>, t+2)}  + ... + \gamma^n Q_{(<s>, t+n)}$$

简单迭代实现

```python
 if len(self.state_list) == self.n:  # 若保存的数据可以进行n步更新
     G = self.Q_table[s1, a1]  # 得到Q(s_{t+n}, a_{t+n})
     for i in reversed(range(self.n)):
         G = self.gamma * G + self.reward_list[i]  # 不断向前计算每一步的回报
         # 如果到达终止状态,最后几步虽然长度不够n步,也将其进行更新
         if done and i > 0:
             s = self.state_list[i]
             a = self.action_list[i]
             self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])
     # n步Sarsa的主要更新步骤
     self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])
```

### 5.2.4 ActorCritic

**迭代时间**：每个动作之后就进行迭代  
**回合内行动策略**：探索+利用  
**特殊构造**：Actor负责探索($Q_{(<s, a>, t)}$), Critic负责评估状态价值($V_{s, t}$)。   
**迭代公式**： 
<font color=darkred>**当前状态行动带来的收益($Q_{(<s, a>, t)}$)加一定比例的行动收益(`td_t`)**</font>;  
行为收益等于**当前行动奖励($R_t$)加上一定比例($\gamma$)的下一状态的价值($V_{s, t+1}$)减去当前状态的价值($V_{s, t}$)**

$$Actor: Q_{(<s, a>, t)} = Q_{(<s, a>, t)} + \alpha * td_t\\ \\ \\
Critic: V_{s, t} = V_{s, t} + \alpha * td_t\\ \\ \\ 
td_t = R_t + \gamma * V_{s, t+1} - V_{s, t}$$

**脚本实现**:

```python
s = env.reset()
done = False
while not done:
    if render:
        env.render()
    a = actor.policy(s)
    n_state, reward, done, info = env.step(a)

    gain = reward + gamma * critic.V[n_state]
    estimated = critic.V[s]
    td = gain - estimated
    actor.Q[s][a] += learning_rate * td
    critic.V[s] += learning_rate * td
    s = n_state

else:
    actor.log(reward)

```
