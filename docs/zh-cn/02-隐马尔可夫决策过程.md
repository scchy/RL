# DataWhale习题回答

## 2.1 为什么在马尔可夫奖励过程中需要有折扣因子（`discount factor`）？

- 马尔可夫过程是带环的，需要避免无穷的奖励
- 权衡及时奖励与未来收益
  - 我们没办法完美模拟环境，对未来的预估不一定准确。折扣因子可以将这个不确定性表达出来。
  - 奖励在设计的时候具有实际价值，我们可以通过设置超参数（`discount factor`, $\gamma$）来调节智能体对即时奖励和未来收益（延迟奖励）的关注度。

## 2.2 为什么矩阵形式的贝尔曼方程（Bellman equation）的解析解比较难解?

- 矩阵的解析解需要对矩阵进行求逆，该过程复杂度是$O(N^3)$
- 转移矩阵随状态空间平方增
  - 1000个状态，则转移矩阵就有$1000^2$这么大

## 2.3 计算贝尔曼方程（Bellman equation）的常见方法以及区别？

1. 蒙特卡罗方法(`(Monte-Carlo Evaluation`)：可用来计算价值函数的值
2. 动态规划方法(`Dynamic Programming`)：可用来计算价值函数的值
3. 时间差分学习(`Temporal Difference)`)（以上两者的结合）

## 2.4 马尔可夫奖励过程（MRP）与马尔可夫决策过程（MDP）的区别？

- 马尔可夫决策过程比马尔可夫奖励过程多了一个决策过程。
  - 状态转移也多了一个条件，即采取行为，从而导致未来的状态的变化: $T( s' | s)$ -> $T(s' | s, a)$。
  - 价值函数也一样多了一个条件: $Q(s)$ -> $Q(s, a)$。
- 两者可以进行相互转换：
  - 条件：已知马尔科夫决策过程 和 一个策略函数
  - 已知策略函数：对于转移函数 $T(s'|s, a)$ 可以计算它在每个状态采取行动的概率。直接将动作进行加和，就可以得到马尔可夫奖励过程。

## 2.5 马尔可夫决策过程里面的状态转移与马尔可夫奖励过程的结构或者计算方面的差异？

- 结构差异：马尔可夫决策过程比马尔可夫奖励过程多了一层行为
- 计算差异：增加了动作策略概率函数 $\pi(s'|s, a)$
  - 马尔可夫`奖励`过程的状态转移（马尔可夫链），转移概率是直接确定的，就是从当前状态通过转移概率得到下一时刻的状态值。
  - 马尔可夫`决策`过程的状态转移，首先要确定采取哪个行动，然后才是进行状态转移

## 2.6 我们如何寻找最优策略，方法有哪些？

1. 穷举法
2. 迭代法
   1. 策略迭代
   2. 价值迭代

