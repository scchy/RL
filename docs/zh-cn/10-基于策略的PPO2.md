


|-|PPO|PPO2|ref|
|-|-|-|-|
|data collect| one episode| several episode(one batch)|
|activation| ReLU | Tanh |
|adv-compute| - | compute adv as one serires | 
|adv-normalize| mini-batch normalize | servel envs-batch normalize | [影响PPO算法性能的10个关键技巧](https://zhuanlan.zhihu.com/p/512327050) |
|Value Function Loss Clipping| - | $L^{V}=max[(V_{\theta_t} - V_{tar})^2, (clip(V_{\theta_t}, V_{\theta_{t-1}}-\epsilon, V_{\theta_{t-1}}+\epsilon))^2]$ |[The 37 Implementation Details of Proximal Policy Optimization](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)|
|optimizer| actor-opt & critic-opt | use common opt |
|loss| actor-loss-backward & critic-loss-backward | loss weight sum |
|paramate-init| - | 1- **hidden layer** orthogonal initialization of weights **$\sqrt{2}$**;   2- **The policy output** layer weights are initialized with the scale of **0.01**;   3- **The value output** layer weights are initialized with the scale of **1.0** |[The 37 Implementation Details of Proximal Policy Optimization](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)|
|training envs| single gym env | SyncVectorEnv |




