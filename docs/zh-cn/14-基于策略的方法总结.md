# 基于策略的DRL方法总结

策略梯度算法和 `Actor-Critic` 算法训练不稳定，所以衍生出基于 `Actor-Critic` 框架的优化的强化学习算法。

## Actor-Policy

> 除PPO, 其他均是最大化当前状态下action的最大价值

|方法| 算法缘由 | ActorNet | action选取 | Loss | 是否有TargetNet | 备注 |
|-|-|-|-|-|-|-|
|PPO-Clip | TRPO(2015)计算过于复杂，提出改进版PPO(2017) 目标函数中进行限制，以保证新的参数和旧的参数的差距不会太大| Actor(state)->`mu,std`| 采样 * max_action |$A_t = \gamma \lambda A_{t-1} + DT_t; \delta = \frac{\pi_ \theta(a|s)}{\pi_{\theta_k}(a|s)};Loss = -min[\delta * A, clip(\delta, 1- \epsilon, 1+ \epsilon) * A]$| 否 |
|DDPG| 吸收DQN的优点，同时弥补PPO这类算法的缺陷(样本效率过低) | Actor(state)-> a | a * max_action | $Loss=-Critic[s_{t}, Actor(s_{t})]$ | 是 | 乘以max_action可直接放在ActorNet中 |
|TD3|DDPG的critic会高估, 从而导致actor策略失败。TD3是增加了三个关键技巧优化DDPG| Actor(state)-> a | a * max_action| $Loss=-Critic.Q1[s_{t}, Actor(s_t)]$ | 是 | 乘以max_action可直接放在ActorNet中 |
|SAC| 除了要最大化累积奖励，还要使得策略更加随机,$\alpha$越大，探索性就越强 | Actor(state)-> `a, log_prob` | a * max_action| $Loss=  e^{log(\alpha)} * \text{log_prob} - min(q_1^v, q_2^v)$ | 是 | $q_i^v=Critic^{tar}_{i}(s_{t}, a)$, $Loss_{\alpha} = \frac{1}{N}\sum (-\text{log_prob} - \text{target_entropy}) * e^{log(\alpha)}$|

## Criticor-Value

> 均是最小化TD-error

|方法| CriticNet | Loss| 是否有TargetNet  | 备注 |
|-|-|-|-|-|
|PPO-Clip | CriticNet(state)->q| $Q_{tar}= reward+\gamma Critic(s_{t+1});Loss=MSE(Critic(s_{t}), Q_{tar})$| 否 |
|DDPG| CriticNet(state, action)->q | $Q_{tar}= reward+\gamma q^{tar}; Loss=MSE(q^{cur}, Q_{tar})$ | 是 | $q^{tar}=Critic^{tar}[s_{t+1}, Actor^{tar}(s_{t+1})];q^{cur}=Critic(s_{t}, Actor(s_{t}))$ |
|TD3| CriticNet(state, action)->q1, q2 | $Q_{tar}= reward+\gamma min(q_1^{tar},q_2^{tar});Loss=MSE(q_1^{cur}, Q_{tar}) + MSE(q_2^{cur}, Q_{tar})$| 是 |$q_1^{tar},q_2^{tar} = Critic^{tar}[s_{t+1}, Acto^{tar}(s_{t+1})+noise];q_1^{cur}, q_2^{cur}=Critic(s_t, Actor(s_t))$  |
|SAC|CriticNet1(state, action)->q1, CriticNet2(state, action)->q2| $Q_{tar}= reward + \gamma[min(q_1^{v+1}, q_2^{v+1}) - e^{log(\alpha)} * \text{log_prob}];Loss_i=MSE(q_i^{cur}, Q_{tar})$| 是 | $q_i^{v+1}=Critic^{tar}_{i}(s_{t+1}, a_{t+1});q_i^{cur}=Critic_{i}(s_{t}, a_{t})$|


## 14.1 环境实验与调参经验

|环境与描述 | 使用方法 | 网络结构 | 环境参数配置 | agent参数配置 | 训练参数配置 | 经验 |
|-|-|-|-|-|-|-|
|[ BipedalWalker-v3 ](state: (24,),action: (4,)(连续 <-1.0 -> 1.0>))| DDPG | Actor[128, 64];Critic[128, 64] | `dict(seed=42)` |`dict(actor_lr=5e-5,critic_lr=1e-3,gamma=0.99,DDPG_kwargs={'tau': 0.05,'sigma': 0.5,'action_bound': 1.0, 'action_low': -1, 'action_high': 1})`|`dict(num_episode=5000, off_buffer_size=20480, off_minimal_size=2048, sample_size=128, max_episode_steps=300)`| 环境state较多，适当增加一些buffer，`num_episode`需要调大,软更新`tau`要设置相对小一点，critic的学习率一般大于actor的学习率 |
|[ BipedalWalkerHardcore-v3 ](state: (24,),action: (4,)(连续 <-1.0 -> 1.0>))| TD3 | Actor[200, 200];Critic[200, 200] | `dict(seed=42)` | `dict(actor_lr=1e-4,critic_lr=3e-4,gamma=0.99,TD3_kwargs={'action_low': -1,'action_high': 1,'tau': 0.005,'delay_freq': 1,'policy_noise': 0.2,'policy_noise_clip': 0.5,'expl_noise': 0.25,'expl_noise_exp_reduce_factor': 0.999})` | `dict(num_episode=7800, off_buffer_size=int(1e6), off_minimal_size=2048, sample_size=256, max_episode_steps=1000)`| 环境复杂多变，需要保存多一些buffer，环境复杂`num_episode`需要调大,软更新`tau`要设置小于0.01 |
|[ Reacher-v4 ](state: (11,),action: (2,)(连续 <-1.0 -> 1.0>))| SAC | Actor[200, 200];Critic[200, 200] | `1-9999`随机 | `dict(actor_lr=1e-5,critic_lr=3e-4,alpha_lr=5e-4,gamma=0.98,SAC_kwargs={'tau: 0.03,'target_entropy': -动作空间,'action_bound': 1})` | `dict(num_episode=5000, off_buffer_size=204800, off_minimal_size=2048, sample_size=256, max_episode_steps=300, max_episode_rewards=2048)`| 任务目标会发生变化的时候需要多保存一些buffer, 训练的`num_episode`需要调大，同时`actor_lr`需要适当调小一下些|
|[ Pusher-v4 ](state: (23,),action: (7,)(连续 <-2.0 -> 2.0>))| SAC | Actor[256, 256];Critic[256, 256] | `1-9999`随机 | `dict(actor_lr=3e-5,critic_lr=5e-4,alpha_lr=5e-3,gamma=0.98,SAC_kwargs={'tau: 0.005,'target_entropy': -动作空间,'action_bound': 2})`|`dict(num_episode=3000, off_buffer_size=204800*4, off_minimal_size=2048 * 10, sample_size=256, max_episode_steps=800, max_episode_rewards=2048)` | 任务目标会发生变化的时候需要多保存一些buffer, 训练的`num_episode`需要调大，增大`actor_lr`同时软更新`tau`需要减小；增大一下探索`alpha_lr`,适当加大actor-critic网络，初始随机探索`off_minimal_size`, 增加保存数据的空间`off_buffer_size`|
|[ CarRacing-v2 ](state: (96, 96, 3),action: (3,)(连续 <-1.0 -> 1.0>))| TD3 | Actor-CNN;Critic-CNN | `1-9999`随机 | `dict(actor_lr=2.5e-4,critic_lr=1e-3,gamma=0.99,TD3_kwargs={'CNN_env_flag': 1, 'pic_shape': env.observation_space.shape, 'env': env, 'action_low': env.action_space.low, 'action_high': env.action_space.high, 'tau': 0.05,  'delay_freq': 1, 'policy_noise': 0.2, 'policy_noise_clip': 0.5, 'expl_noise': 0.5, 'expl_noise_exp_reduce_factor':  1 - 1e-4})`|`dict(num_episode=15000, off_buffer_size=1024*100, off_minimal_size=256, sample_size=128, max_episode_steps=1200, max_episode_rewards=50000)` | 环境其实并不是非常复杂，主要取决于CNN网络对特征的提取能力，不过需要多保存一些buffer(`off_buffer_size`), 训练的`num_episode`需要调足够大让模型充分训练，增大`actor_lr`同时软更新`tau`需要减小，初始随机探索`off_minimal_size`|
|[ InvertedPendulum-v4 ](state: (4,),action: (1,)(连续 <-3.0 -> 3.0>))| TD3 | Actor[200, 200];Critic[200, 200] | `1-9999`随机 | `dict(actor_lr=1e-4,critic_lr=3e-4,gamma=0.99,TD3_kwargs={'CNN_env_flag': 0, 'action_low': env.action_space.low, 'action_high': env.action_space.high, 'tau': 0.005,  'delay_freq': 1, 'policy_noise': 0.2, 'policy_noise_clip': 0.5, 'expl_noise': 0.5, 'expl_noise_exp_reduce_factor':  1 - 1e-4})`|`dict(num_episode=1000, off_buffer_size=int(1e6), off_minimal_size=512, sample_size=128, max_episode_steps=1000, max_episode_rewards=1000)` | 需要多保存一些buffer(`off_buffer_size`), 环境较简单， `num_episode`设置稍微大一点就行，但是`actor_lr`和`tau`需要减小|
|[ HalfCheetah-v4 ](state: (17,),action: (6,)(连续 <-1.0 -> 1.0>))| PPO | Actor[256, 256];Critic[256, 256] | `seed=42` | `dict(actor_lr=5e-4,critic_lr=1e-3,gamma=0.98,PPO_kwargs={'lmbda': 0.9, 'eps': 0.25, 'k_epochs': 10, 'sgd_batch_size': 128, 'minibatch_size': 12, 'actor_bound': 1})`|`dict(num_episode=10000, off_buffer_size=102400, sample_size=128, max_episode_steps=500)` | 环境相对还好稍微增Actor和Critic的网络宽度，减少`minibatch_size` |
|[ Hopper-v4 ](state: (11,),action: (3,)(连续 <-1.0 -> 1.0>))| PPO2 | Actor[256, 256, 256];Critic[256, 256, 256] | `seed=42` | `dict(actor_lr=1.5e-4,critic_lr=5.5e-4,gamma=0.99,PPO_kwargs={'lmbda': 0.9, 'eps': 0.2, 'k_epochs': 4, 'sgd_batch_size': 128, 'minibatch_size': 12, 'dist_type': 'beta', 'actor_bound': 1})`|`dict(num_episode=12500, off_buffer_size=512, off_minimal_size=510, max_episode_steps=500)` | PPO2主要是收集多轮的结果序列进行训练，增加训练轮数，适当降低学习率，稍微增Actor和Critic的网络深度 |


## 14.2 实验效果

|环境与描述 | 使用方法 | 配置| 效果|
|-|-|-|-|
|[ BipedalWalker-v3 ](state: (24,),action: (4,)(连续 <-1.0 -> 1.0>))| DDPG | 上述对应配置| ![DDPG](../pic/DDPG_BipedalWalker.gif) |
|[ BipedalWalkerHardcore-v3 ](state: (24,),action: (4,)(连续 <-1.0 -> 1.0>))| TD3 | 上述对应配置| ![TD3](../pic/TD3_perf_new.gif) |
|[ Reacher-v4 ](state: (11,),action: (2,)(连续 <-1.0 -> 1.0>))| SAC | 上述对应配置| ![SAC](../pic/SAC_Reacher-v4.gif) |
|[ Pusher-v4 ](state: (23,),action: (7,)(连续 <-2.0 -> 2.0>))| SAC | 上述对应配置| ![SAC-2](../pic/SAC_Pusher-v4.gif) |
|[ CarRacing-v2 ](state: (96, 96, 3),action: (3,)(连续 <-1.0 -> 1.0>))| TD3 | 上述对应配置| ![TD3-car](../pic/TD3_CarRacing-v2.gif) |
|[ InvertedPendulum-v4 ](state: (4,),action: (1,)(连续 <-3.0 -> 3.0>))| TD3 | 上述对应配置| ![TD3-InvertedPendulum](../pic/TD3_InvertedPendulum-v4.gif) |
|[ HalfCheetah-v4 ](state: (17,),action: (6,)(连续 <-1.0 -> 1.0>))| PPO | 上述对应配置| ![PPO-PPO_HalfCheetah-v4](../pic/PPO_HalfCheetah-v4.gif) |
|[ Hopper-v4 ](state: (11,),action: (3,)(连续 <-1.0 -> 1.0>))| PPO2 | 上述对应配置| ![PPO2-PPO2_Hopper-v4](../pic/PPO2_Hopper-v4.gif) |
|[ ALE/DemonAttack-v5 ](state: (210, 160, 3),action: 6(离散 ))| PPO2 | 上述对应配置| |
|[ ALE/AirRaid-v5 ](state: (250, 160, 3),action: 6(离散 ))| PPO2 | 上述对应配置| |


## 14.3 环境实验脚本简述

**BipedalWalker-v3**
详细看[github test_DDPG 脚本](https://github.com/scchy/RL/blob/main/src/test/test_DDPG.py)

```python

import os
import gym
import torch
from RLAlgo.TD3 import TD3
from RLUtils import train_off_policy, play, Config, gym_env_desc


env_name = 'BipedalWalker-v3'
gym_env_desc(env_name)
env = gym.make(env_name)
path_ = os.path.dirname(__file__)
cfg = Config(
    env, 
    # 环境参数
    # save_path=r'D:\TMP\ddpg_BipedalWalker-v3_test_actor-v2.ckpt', # 1000次 291
    save_path=os.path.join(path_, "test_models" ,'DDPG_BipedalWalker-v3_test_actor-0.ckpt'), 
    seed=42,
    # 网络参数
    actor_hidden_layers_dim=[128, 64],
    critic_hidden_layers_dim=[128, 64],
    # agent参数
    actor_lr=5e-5, # 3e-5,
    critic_lr=1e-3, # 5e-4
    gamma=0.99,
    # 训练参数
    num_episode=5000,
    sample_size=128,
    off_buffer_size=20480,
    off_minimal_size=2048,
    max_episode_rewards=900,
    max_episode_steps=300,
    # agent 其他参数
    DDPG_kwargs={
        'tau': 0.05, # soft update parameters
        'sigma': 0.5, # noise
        'action_bound': 1.0,
        'action_low': env.action_space.low[0],
        'action_high': env.action_space.high[0],
    }
)
agent = DDPG(
    state_dim=cfg.state_dim,
    actor_hidden_layers_dim=cfg.actor_hidden_layers_dim,
    critic_hidden_layers_dim=cfg.critic_hidden_layers_dim,
    action_dim=cfg.action_dim,
    actor_lr=cfg.actor_lr,
    critic_lr=cfg.critic_lr,
    gamma=cfg.gamma,
    DDPG_kwargs=cfg.DDPG_kwargs,
    device=cfg.device
)
agent.train = True
train_off_policy(env, agent, cfg, done_add=False)

```


**BipedalWalkerHardcore-v3**
详细看[github test_TD3 脚本](https://github.com/scchy/RL/blob/main/src/test/test_TD3.py), *需要注意训练num_episode 需要设置更多(1W+)*

```python

import os
import gym
import torch
from RLAlgo.TD3 import TD3
from RLUtils import train_off_policy, play, Config, gym_env_desc


def reward_func(r, d):
    if r <= -100:
        r = -1
        d = True
    else:
        d = False
    return r, d


env_name = 'BipedalWalkerHardcore-v3'
gym_env_desc(env_name)
env = gym.make(env_name)
print("gym.__version__ = ", gym.__version__ ) #0.26.2
path_ = os.path.dirname(__file__)
cfg = Config(
    env, 
    # 环境参数
    save_path=os.path.join(path_, "test_models" ,'TD3_BipedalWalkerHardcore-v3_test_actor-3.ckpt'), 
    seed=42,
    # 网络参数
    actor_hidden_layers_dim=[200, 200],
    critic_hidden_layers_dim=[200, 200],
    # agent参数
    actor_lr=1e-4,
    critic_lr=3e-4,
    gamma=0.99,
    # 训练参数
    num_episode=5000,
    sample_size=256,
    # 环境复杂多变，需要保存多一些buffer
    off_buffer_size=int(1e6),
    off_minimal_size=2048,
    max_episode_rewards=1000,
    max_episode_steps=1000,
    # agent 其他参数
    TD3_kwargs={
        'action_low': env.action_space.low[0],
        'action_high': env.action_space.high[0],
        # soft update parameters
        'tau': 0.005, 
        # trick2: Delayed Policy Update
        'delay_freq': 1,
        # trick3: Target Policy Smoothing
        'policy_noise': 0.2,
        'policy_noise_clip': 0.5,
        # exploration noise
        'expl_noise': 0.25,
        # 探索的 noise 指数系数率减少 noise = expl_noise * expl_noise_exp_reduce_factor^t
        'expl_noise_exp_reduce_factor': 0.999
    }
)
agent = TD3(
    state_dim=cfg.state_dim,
    actor_hidden_layers_dim=cfg.actor_hidden_layers_dim,
    critic_hidden_layers_dim=cfg.critic_hidden_layers_dim,
    action_dim=cfg.action_dim,
    actor_lr=cfg.actor_lr,
    critic_lr=cfg.critic_lr,
    gamma=cfg.gamma,
    TD3_kwargs=cfg.TD3_kwargs,
    device=cfg.device
)
train_off_policy(env, agent, cfg, done_add=False, reward_func=reward_func)

```

**Reacher-v4**
详细看[github test_sac 脚本: Reacher-v4](https://github.com/scchy/RL/blob/main/src/test/test_sac.py)

```python
import os
from os.path import dirname
import sys
import gymnasium as gym
import torch
dir_ = dirname(dirname(__file__))
print(dir_)
sys.path.append(dir_)
from RLAlgo.SAC import SAC
from RLUtils import train_off_policy, play, Config, gym_env_desc


def sac_Reacher_v4_test():
    """    
    final fix:
        pip install gymnasium
        pip install gymnasium[mujoco]
    """
    env_name = 'Reacher-v4'
    gym_env_desc(env_name)
    env = gym.make(env_name)
    print("gym.__version__ = ", gym.__version__ )
    path_ = os.path.dirname(__file__)
    cfg = Config(
        env, 
        num_episode=5000,
        save_path=os.path.join(path_, "test_models" ,'SAC_Reacher-v4_test_actor-0.ckpt'), 
        actor_hidden_layers_dim=[200, 200],
        critic_hidden_layers_dim=[200, 200],
        actor_lr=1e-5,
        critic_lr=3e-4,
        sample_size=256,
        off_buffer_size=204800,
        off_minimal_size=2048,
        max_episode_rewards=2048,
        max_episode_steps=300,
        gamma=0.98,
        SAC_kwargs={
            'tau': 0.03, # soft update parameters
            'target_entropy': -env.action_space.shape[0],
            'action_bound': 1.0
        }
    )
    agent = SAC(
        state_dim=cfg.state_dim,
        actor_hidden_layers_dim=cfg.actor_hidden_layers_dim,
        critic_hidden_layers_dim=cfg.critic_hidden_layers_dim,
        action_dim=cfg.action_dim,
        actor_lr=cfg.actor_lr,
        critic_lr=cfg.critic_lr,
        alpha_lr=5e-4,
        gamma=cfg.gamma,
        SAC_kwargs=cfg.SAC_kwargs,
        device=cfg.device
    )
    # train_off_policy(env, agent, cfg, train_without_seed=True)
    try:
        agent.target_q.load_state_dict(
            torch.load(cfg.save_path)
        )
    except Exception as e:
        agent.actor.load_state_dict(
            torch.load(cfg.save_path, map_location='cpu')
        )
    play(gym.make(env_name, render_mode='human'), agent, cfg, episode_count=2, play_without_seed=True)

```

**Pusher-v4**

详细看[github test_sac 脚本: Pusher-v4](https://github.com/scchy/RL/blob/main/src/test/test_sac.py)

```python

def sac_Pusher_v4_test():
    env_name = 'Pusher-v4'
    gym_env_desc(env_name)
    env = gym.make(env_name)
    print("gym.__version__ = ", gym.__version__ )
    path_ = os.path.dirname(__file__)
    cfg = Config(
        env, 
        num_episode=3000,
        save_path=os.path.join(path_, "test_models" ,'SAC_Pusher-v4_test_actor-1.ckpt'), 
        actor_hidden_layers_dim=[256, 256],
        critic_hidden_layers_dim=[256, 256],
        actor_lr=3e-5,
        critic_lr=5e-4,
        sample_size=256,
        off_buffer_size=204800*4,
        off_minimal_size=2048 * 10,
        max_episode_rewards=2048,
        max_episode_steps=800,
        gamma=0.98,
        SAC_kwargs={
            'tau': 0.005, # soft update parameters
            'target_entropy': -torch.prod(torch.Tensor(env.action_space.shape)).item(),
            'action_bound': 2.0
        }
    )
    agent = SAC(
        state_dim=cfg.state_dim,
        actor_hidden_layers_dim=cfg.actor_hidden_layers_dim,
        critic_hidden_layers_dim=cfg.critic_hidden_layers_dim,
        action_dim=cfg.action_dim, 
        actor_lr=cfg.actor_lr,
        critic_lr=cfg.critic_lr,
        alpha_lr=5e-3,
        gamma=cfg.gamma,
        SAC_kwargs=cfg.SAC_kwargs,
        device=cfg.device
    )
    # Episode [ 1867 / 3000|(seed=8526) ]:  62%|██████████▌      | 1866/3000 [4:35:15<3:16:03, 10.37s/it, steps=800, lastMeanRewards=-149.66, BEST=-119.49]
    # train_off_policy(env, agent, cfg, train_without_seed=True)
    agent.actor.load_state_dict(
        torch.load(cfg.save_path, map_location='cpu')
    )
    play(gym.make(env_name, render_mode='human'), agent, cfg, episode_count=2, play_without_seed=True)


```

**CarRacing-v2**
详细看[github test_TD3.py : CarRacing_TD3_test()](https://github.com/scchy/RL/blob/main/src/test/test_TD3.py)

一些技巧(tricks):

1. 环境观察与调整:  
   1. 跳帧：一个action执行5个step(5桢)
   2. 图像裁剪->图像转灰度->图像归一化
   3. 增加车驶离赛道的判断(`judge_out_of_route`)
   4. 对多个输出进行通道叠加`FrameStack`
2. CNN网络
   1. 解决梯度消失问题(`Vanishing gradient problem`)
   2. 池化技巧：MaxPool2d + AvgPool2d
   3. actor网络: 增加LayerNorm；输出的时候进行maxMinScale
   4. criticor网络: 进行observe和action concat 之前对action进行线性变换（一定程度解决梯度消失 及 原地转圈）
3. 训练`train_off_policy`
   1. 在训练的过程中增加测试阶段：每隔`test_ep_freq`进行测试
   2. 基于多次测试的奖励均值进行最佳模型参数保存
4. TD3算法调整
   1. policy_noise: 分布调整为(mean=0, std=每个维度动作范围) * self.policy_noise
   2. expl_noise: 分布调整为(mean=0, std=每个维度动作范围) * self.train_noise

```python
# Episode [ 10061 / 15000|(seed=8409) ]:  67%|██████▋   | 10060/15000 [9:03:17<2:44:32,  2.00s/it, steps=23, lastMeanRewards=140.40, BEST=545.37, bestTestReward=810.89]

def CarRacing_TD3_test():
    """
    policyNet: 
    valueNet: 
    reference: https://hiddenbeginner.github.io/study-notes/contents/tutorials/2023-04-20_CartRacing-v2_DQN.html
    """
    env_name = 'CarRacing-v2'
    gym_env_desc(env_name)
    env = gym.make(env_name)
    env = FrameStack(
        ResizeObservation(
            GrayScaleObservation(CarV2SkipFrame(env, skip=5)), 
            shape=84
        ), 
        num_stack=4
    )
    print("gym.__version__ = ", gym.__version__ )
    path_ = os.path.dirname(__file__)
    cfg = Config(
        env, 
        # 环境参数
        save_path=os.path.join(path_, "test_models" ,'TD3_CarRacing-v2_test2-3'), 
        seed=42,
        # 网络参数
        actor_hidden_layers_dim=[128],
        critic_hidden_layers_dim=[128],
        actor_lr=2.5e-4,
        critic_lr=1e-3,

        gamma=0.99,
        # 训练参数
        num_episode=15000,
        sample_size=128,
        # 环境复杂多变，需要保存多一些buffer
        off_buffer_size=1024*100,  
        off_minimal_size=256,
        max_episode_rewards=50000,
        max_episode_steps=1200, # 200
        # agent 其他参数
        TD3_kwargs={
            'CNN_env_flag': 1,
            'pic_shape': env.observation_space.shape,
            "env": env,
            'action_low': env.action_space.low,
            'action_high': env.action_space.high,
            # soft update parameters
            'tau': 0.05, 
            # trick2: Delayed Policy Update
            'delay_freq': 1,
            # trick3: Target Policy Smoothing
            'policy_noise': 0.2,
            'policy_noise_clip': 0.5,
            # exploration noise
            'expl_noise': 0.5,
            # 探索的 noise 指数系数率减少 noise = expl_noise * expl_noise_exp_reduce_factor^t
            'expl_noise_exp_reduce_factor':  1 - 1e-4
        }
    )
    agent = TD3(
        state_dim=cfg.state_dim,
        actor_hidden_layers_dim=cfg.actor_hidden_layers_dim,
        critic_hidden_layers_dim=cfg.critic_hidden_layers_dim,
        action_dim=cfg.action_dim,
        actor_lr=cfg.actor_lr,
        critic_lr=cfg.critic_lr,
        gamma=cfg.gamma,
        TD3_kwargs=cfg.TD3_kwargs,
        device=cfg.device
    )
    # 载入再学习 (The same param with test2-3 training (without train_action & test_ep_freq in train_off_policy))
    train_1 = os.path.join(path_, "test_models" ,'TD3_CarRacing-v2_test2-2')
    agent.load_model(train_1)
    agent.train()
    train_off_policy(env, agent, cfg, done_add=False, train_without_seed=True, wandb_flag=False, test_ep_freq=100)
    agent.load_model(cfg.save_path)
    agent.eval()
    env = gym.make(env_name, render_mode='human') # 
    env = FrameStack(
        ResizeObservation(
            GrayScaleObservation(CarV2SkipFrame(env, skip=5)), 
            shape=84
        ), 
        num_stack=4
    )
    play(env, agent, cfg, episode_count=2)

```


**InvertedPendulum-v4**
详细看[github test_TD3.py : InvertedPendulum_TD3_test()](https://github.com/scchy/RL/blob/main/src/test/test_TD3.py)
> almost same in `InvertedDoublePendulum-v4`

```python

def InvertedPendulum_TD3_test():
    """
    policyNet: 
    valueNet: 
    """
    env_name = 'InvertedPendulum-v4'
    gym_env_desc(env_name)
    env = gym.make(env_name)
    print("gym.__version__ = ", gym.__version__ )
    path_ = os.path.dirname(__file__)
    cfg = Config(
        env, 
        # 环境参数
        save_path=os.path.join(path_, "test_models" ,'TD3_InvertedPendulum-v4_test1.ckpt'), 
        seed=42,
        # 网络参数
        actor_hidden_layers_dim=[200, 200],
        critic_hidden_layers_dim=[200, 200],
        # agent参数
        actor_lr=1e-4,
        critic_lr=3e-4,
        gamma=0.99,
        # 训练参数
        num_episode=1000,
        sample_size=128,
        # 环境复杂多变，需要保存多一些buffer
        off_buffer_size=int(1e6),
        off_minimal_size=512,
        max_episode_rewards=1000,
        max_episode_steps=1000,
        # agent 其他参数
        TD3_kwargs={
            'CNN_env_flag': 0,
            'action_low': env.action_space.low,
            'action_high': env.action_space.high,
            # soft update parameters
            'tau': 0.005, 
            # trick2: Delayed Policy Update
            'delay_freq': 1,
            # trick3: Target Policy Smoothing
            'policy_noise': 0.2,
            'policy_noise_clip': 0.5,
            # exploration noise
            'expl_noise': 0.5,
            # 探索的 noise 指数系数率减少 noise = expl_noise * expl_noise_exp_reduce_factor^t
            'expl_noise_exp_reduce_factor': 1 - 1e-4,
            'off_minimal_size': 4096
        }
    )
    agent = TD3(
        state_dim=cfg.state_dim,
        actor_hidden_layers_dim=cfg.actor_hidden_layers_dim,
        critic_hidden_layers_dim=cfg.critic_hidden_layers_dim,
        action_dim=cfg.action_dim,
        actor_lr=cfg.actor_lr,
        critic_lr=cfg.critic_lr,
        gamma=cfg.gamma,
        TD3_kwargs=cfg.TD3_kwargs,
        device=cfg.device
    )
    agent.train()
    train_off_policy(env, agent, cfg, done_add=False, train_without_seed=True, wandb_flag=False, test_ep_freq=100)
    agent.load_model(cfg.save_path)
    agent.eval()
    play_env = gym.make(env_name, render_mode='human')
    play(play_env, agent, cfg, episode_count=2, render=True)

```


**HalfCheetah-v4**
详细看[github test_PPO.py : HalfCheetah_v4_ppo_test()](https://github.com/scchy/RL/blob/main/src/test/test_PPO.py)

```python

def HalfCheetah_v4_ppo_test():
    """
    policyNet: 
    valueNet: 
    """
    # A:[128, 64] C:[64, 32]
    env_name = 'HalfCheetah-v4'
    gym_env_desc(env_name)
    print("gym.__version__ = ", gym.__version__ )
    path_ = os.path.dirname(__file__)
    env = gym.make(env_name, forward_reward_weight=2.0)
    cfg = Config(
        env, 
        # 环境参数
        save_path=os.path.join(path_, "test_models" ,'PPO_HalfCheetah-v4_test1'), 
        seed=42,
        # 网络参数
        actor_hidden_layers_dim=[256, 256],
        critic_hidden_layers_dim=[256, 256],
        # agent参数
        actor_lr=5e-4,
        critic_lr=1e-3,
        gamma=0.98,
        # 训练参数
        num_episode=10000,
        off_buffer_size=102400, #204800,
        max_episode_steps=500,
        # agent其他参数
        PPO_kwargs={
            'lmbda': 0.9,
            'eps': 0.2, # clip eps
            'k_epochs': 10,
            'sgd_batch_size': 128,
            'minibatch_size': 12,
            'actor_nums': 3,
            'actor_bound': 1
        }
    )
    agent = PPO(
        state_dim=cfg.state_dim,
        actor_hidden_layers_dim=cfg.actor_hidden_layers_dim,
        critic_hidden_layers_dim=cfg.critic_hidden_layers_dim,
        action_dim=cfg.action_dim,
        actor_lr=cfg.actor_lr,
        critic_lr=cfg.critic_lr,
        gamma=cfg.gamma,
        PPO_kwargs=cfg.PPO_kwargs,
        device=cfg.device,
        reward_func=r_func
    )
    # agent.train()
    # train_on_policy(env, agent, cfg, wandb_flag=False, train_without_seed=False, test_ep_freq=500)
    agent.load_model(cfg.save_path)
    agent.eval()
    env_ = gym.make(env_name, render_mode='human')
    play(env_, agent, cfg, episode_count=2, render=True)

```


**Hopper-v4**
详细看[github test_PPO.py : Hopper_v4_ppo2_test()](https://github.com/scchy/RL/blob/main/src/test/test_PPO.py)

```python

# Hopper-v4
def Hopper_v4_ppo2_test():
    """
    policyNet: 
    valueNet: 
    """
    env_name = 'Hopper-v4'
    gym_env_desc(env_name)
    print("gym.__version__ = ", gym.__version__ )
    path_ = os.path.dirname(__file__) 
    env = gym.make(
        env_name, 
        exclude_current_positions_from_observation=True,
        # healthy_reward=0
    )
    cfg = Config(
        env, 
        # 环境参数
        save_path=os.path.join(path_, "test_models" ,'PPO_Hopper-v4_test2'), 
        seed=42,
        # 网络参数
        actor_hidden_layers_dim=[256, 256, 256],
        critic_hidden_layers_dim=[256, 256, 256],
        # agent参数
        actor_lr=1.5e-4,
        critic_lr=5.5e-4,
        gamma=0.99,
        # 训练参数
        num_episode=12500,
        off_buffer_size=512,
        off_minimal_size=510,
        max_episode_steps=500,
        PPO_kwargs={
            'lmbda': 0.9,
            'eps': 0.25,
            'k_epochs': 4, 
            'sgd_batch_size': 128,
            'minibatch_size': 12, 
            'actor_bound': 1,
            'dist_type': 'beta'
        }
    )
    agent = PPO2(
        state_dim=cfg.state_dim,
        actor_hidden_layers_dim=cfg.actor_hidden_layers_dim,
        critic_hidden_layers_dim=cfg.critic_hidden_layers_dim,
        action_dim=cfg.action_dim,
        actor_lr=cfg.actor_lr,
        critic_lr=cfg.critic_lr,
        gamma=cfg.gamma,
        PPO_kwargs=cfg.PPO_kwargs,
        device=cfg.device,
        reward_func=None
    )
    # agent.train()
    # train_on_policy(env, agent, cfg, wandb_flag=False, train_without_seed=True, test_ep_freq=1000, 
    #                 online_collect_nums=cfg.off_buffer_size,
    #                 test_episode_count=5)
    agent.load_model(cfg.save_path)
    agent.eval()
    env_ = gym.make(env_name, 
                    exclude_current_positions_from_observation=True,
                    # render_mode='human'
                    )
    play(env_, agent, cfg, episode_count=3, play_without_seed=True, render=False)

```



**ALE/DemonAttack-v5**
详细看[github test_PPO_atari.py : DemonAttack_v5_ppo2_test()](https://github.com/scchy/RL/blob/main/src/test/test_PPO_atari.py)

环境需要进行一些处理：
```python
def make_atari_env(env_id, episod_life=True, clip_reward=True, **kwargs):
    def thunk():
        env = gym.make(env_id, **kwargs)
        env = gym.wrappers.RecordEpisodeStatistics(env)
        env = baseSkipFrame(env, skip=5, start_skip=30)
        if episod_life:
            env = EpisodicLifeEnv(env)
        if "FIRE" in env.unwrapped.get_action_meanings():
            env = FireResetEnv(env)
        if clip_reward:
            env = ClipRewardEnv(env)
        env = ResizeObservation(GrayScaleObservation(env), shape=84)
        env = gym.wrappers.FrameStack(env, 4)
        return env
    return thunk
```
训练示例
```python
def DemonAttack_v5_ppo2_test():
    """
    policyNet: 
    valueNet: 
    """
    env_name = 'ALE/DemonAttack-v5' 
    env_name_str = env_name.replace('/', '-')
    num_envs = 10
    gym_env_desc(env_name)
    print("gym.__version__ = ", gym.__version__ )
    path_ = os.path.dirname(__file__)
    envs = gym.vector.SyncVectorEnv(
        [make_atari_env(env_name) for _ in range(num_envs)]
    )
    dist_type = 'beta'
    cfg = Config(
        envs, 
        # 环境参数
        save_path=os.path.join(path_, "test_models" ,f'PPO2_{env_name_str}-2'), 
        seed=202404,
        # 网络参数 Atria-CNN + MLP
        actor_hidden_layers_dim=[200],
        critic_hidden_layers_dim=[200],
        # agent参数
        actor_lr=1.5e-4, 
        gamma=0.99,
        # 训练参数
        num_episode=1800, 
        off_buffer_size=500, # batch_size = off_buffer_size * num_env
        max_episode_steps=500,
        PPO_kwargs={
            'cnn_flag': True,
            'continue_action_flag': False,

            'lmbda': 0.95,
            'eps': 0.2,
            'k_epochs': 2,  # update_epochs
            'sgd_batch_size': 256, # 256, # 128,  # 1024, # 512, # 256,  
            'minibatch_size': 200, # 200, # 32,  # 900,  # 256, # 128,  
            'act_type': 'relu',
            'dist_type': dist_type,
            'critic_coef': 1.0,
            'max_grad_norm': 0.5, 
            'clip_vloss': True,
            # 'min_adv_norm': True,

            'anneal_lr': False,
            'num_episode': 3000
        }
    )
    cfg.num_envs = num_envs
    minibatch_size = cfg.PPO_kwargs['minibatch_size']
    max_grad_norm = cfg.PPO_kwargs['max_grad_norm']
    cfg.trail_desc = f"actor_lr={cfg.actor_lr},minibatch_size={minibatch_size},max_grad_norm={max_grad_norm},hidden_layers={cfg.actor_hidden_layers_dim}",
    # {'P25': 0.054308490827679634, 'P50': 10.356741905212402, 'P75': 803.9899291992188, 'P95': 3986.836511230468, 'P99': 8209.22970703126}
    agent = PPO2(
        state_dim=cfg.state_dim,
        actor_hidden_layers_dim=cfg.actor_hidden_layers_dim,
        critic_hidden_layers_dim=cfg.critic_hidden_layers_dim,
        action_dim=cfg.action_dim,
        actor_lr=cfg.actor_lr,
        critic_lr=cfg.critic_lr,
        gamma=cfg.gamma,
        PPO_kwargs=cfg.PPO_kwargs,
        device=cfg.device,
        reward_func=None, # lambda r: (r + 10.0)/10.0
    )
    # agent.train()
    # ppo2_train(envs, agent, cfg, wandb_flag=True, wandb_project_name=f"PPO2-{env_name_str}",
    #                 train_without_seed=False, test_ep_freq=cfg.off_buffer_size * 10, 
    #                 online_collect_nums=cfg.off_buffer_size,
    #                 test_episode_count=10)
    agent.load_model(cfg.save_path)
    agent.eval()
    env = make_atari_env(env_name, clip_reward=False)() #, render_mode='human')()
    # env = make_atari_env(env_name, clip_reward=True)()
    cfg.max_episode_steps = 1620 
    play(env, agent, cfg, episode_count=3, play_without_seed=False, render=False, ppo_train=True)

```


**ALE/AirRaid-v5**
详细看[github test_PPO_atari.py : AirRaid_v5_ppo2_test()](https://github.com/scchy/RL/blob/main/src/test/test_PPO_atari.py)

```python

```