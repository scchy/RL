
## DT: Decision Transformer: Reinforcement Learning via Sequence Modeling

paper Link: [DT: Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/pdf/2106.01345)
GITHUB Link:
- [官方博客页面-google berkeley](https://sites.google.com/berkeley.edu/decision-transformer)
- [Github: kzl/decision-transformer](https://github.com/kzl/decision-transformer/blob/master/gym/decision_transformer/models/decision_transformer.py)

state-of-the-art model-free **offline RL baselines** on Atari, OpenAI Gym, and Key-to-Door tasks.

补全Paper内容 + code解析

比较GPT2 VS DecisionTransformerGPT2

本项目中是怎样实现的 整体流程

## DecisionTransformerModel VS GPT2Model

| 维度    | GPT2Model                                     | DecisionTransformerModel          |
| ----- | --------------------------------------------- | --------------------------------- |
| 网络结构  | 原生 GPT-2 Decoder（Masked Self-Attention + FFN） | 复用 **完全相同** 的 GPT-2 Decoder 层     |
| 层数/宽度 | 默认 12 层 768 隐维                                | 继承同一权重，但可通过 `config.n_layer` 随意改  |
| 位置编码  | 学习式绝对位置       | 忽略，全部设置为0 |
| 序列格式     | `[token1, token2, …]` | `[r1, s1, a1, r2, s2, a2, …]` <br>每步 3 个 token（return-to-go, state, action） |
| 嵌入层      | 仅 Token + Pos         | 额外 **ReturnEmbeddings + StateEmbeddings + ActionEmbeddings**                |
| 损失 | 语言建模交叉熵：预测下一个 token | 回归 MSE：仅预测 **action token** |
| 标签 | 输入右移 1 位            | 输入保持不变，仅对 $a_t$ 位置计算损失       |


