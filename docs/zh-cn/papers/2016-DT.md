
## DT: Decision Transformer: Reinforcement Learning via Sequence Modeling

paper Link: [DT: Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/pdf/2106.01345)
GITHUB Link:
- [官方博客页面-google berkeley](https://sites.google.com/berkeley.edu/decision-transformer)
- [Github: kzl/decision-transformer](https://github.com/kzl/decision-transformer/blob/master/gym/decision_transformer/models/decision_transformer.py)

state-of-the-art model-free **offline RL baselines** on Atari, OpenAI Gym, and Key-to-Door tasks.

补全Paper内容 + code解析

### 1- 简介

最新研究表明，Transformer 模型能够在大规模场景下对高维语义概念分布进行建模，并已在语言任务上实现有效的零样本泛化，以及超出训练分布的图像生成。鉴于这类模型在多种任务中的成功应用，我们尝试将其用于以强化学习（RL）形式化后的序列决策问题。
将offline-RL的建模问题适配transformer框架。训练states, actions, returns构成的序列，将策略简化成自回归生成建模。我们可以通过选择期望的回报（return）标记，来指定策略的“专长”——即要查询的“技能”——从而将其作为生成的提示。
示例，训练图的最短路径:
我们将问题转化为强化学习任务。训练数据集由随机游走轨迹及每个节点的“剩余回报”（returns-to-go）构成（中间图）。给定起始状态，并让模型在每个节点生成最大可能的回报，Decision Transformer 即可依次输出最优路径。

[pic]


### 3- 模型方法

#### 训练轨迹定义
 $\tau = (\hat{R_1}, s_1, a_1, \hat{R_2}, s_2, a_2, ..., \hat{R_T}, s_T, a_T)$
- $\hat{R_t}=\sum_{t^\prime=t}^T r_{t^\prime}$即后续的总回报

#### 测试
- 初始化
  - 设定期望总回报，即$\hat{R_1}$
  - 环境reset, `state, _ = env.reset()`
  - action设置成全为0的向量
- 开始环境交互:
  - 基于轨迹返回action, 并进行交互`n_state, r, terminated, truncated, info = env.step(a)`
  - 轨迹调整, 合并state, 合并action, 合并return-to-go: 新的值为设定值减去奖励 $\hat{R_1} - r$

#### 模型基本架构
我们将最近 K 个时间步输入 Decision Transformer，共 3K=(1K*3) 个 token（每个模态各一个：return-to-go、state 和 action）。
为了获得 token 嵌入，我们为每种模态分别学习一个线性层，将原始输入映射到嵌入维度，随后进行层归一化。
```python
# 额外的Embedding
class DecisionTransformer:
    def __init__(self):
        ...
        self.embed_return = nn.Linear(1, hidden_size)
        self.embed_state = nn.Linear(self.state_dim, hidden_size)
        self.embed_action = nn.Linear(self.act_dim, hidden_size)
        self.embed_timestep = nn.Embedding(max_ep_len, hidden_size)
        ...

    def forward(self):
        ...
        # 输入维度 [B, K, dim]
        state_embeddings = self.embed_state(states)
        action_embeddings = self.embed_action(actions)
        returns_embeddings = self.embed_return(returns_to_go)
        time_embeddings = self.embed_timestep(timesteps)

        state_embeddings = state_embeddings + time_embeddings
        action_embeddings = action_embeddings + time_embeddings
        returns_embeddings = returns_embeddings + time_embeddings
        # this makes the sequence look like (R_1, s_1, a_1, R_2, s_2, a_2, ...)
        #  my-concat: [B//K, K, dim] -> [B//K, 3*K, dim]
        stacked_inputs = torch.concat(
            (returns_embeddings, state_embeddings, action_embeddings),
            dim=1
        )
        stacked_inputs = self.embed_ln(stacked_inputs)
        ...
        tf_out = self.transformer(
            inputs_embeds=stacked_inputs,
            attention_mask=stacked_attention_mask,
            position_ids=torch.zeros(stacked_attention_mask.shape, device=stacked_inputs.device, dtype=torch.long),
        )
        x = tf_out['last_hidden_state']
        ...
```
若环境输入为视觉数据，则用卷积编码器处理 state，而非线性层。
此外，我们为每个时间步学习一个嵌入并加到对应 token 上——注意这与 Transformer 标准的位置嵌入不同，因为单个时间步对应三个 token。
随后，这些 token 被送入 GPT 模型，通过自回归建模来预测未来的 action token。

#### 训练

可以直接看原文中的伪代码图片
[pic]

### 4- 和一些Offline RL基准方法进行比较

Atari 游戏上 [Pong环境`K = 50`, 其他的均设置一样`K = 30`]
[pic]

mojco 环境上
[pic]

### 5- 探讨
 把 RL 问题变成条件序列生成后，Transformer 的长上下文、分布建模与无优化目标特性，天然解决了离线 RL 的泛化、信用分配与保守正则难题。

#### 5.1 DT 不等于 BC
提出 %BC（只在高回报子集上做 BC）作为对照。
结论：**DT 在数据量少的 Atari 仍能超越 %BC，说明它利用全部轨迹做泛化，而非简单克隆子集。**
[PIC]

#### 5.2 DT 对于return-to-go分布建模精准
把目标 return 当作条件，可视化实际回报与目标回报的线性相关性。
结论：**DT 能精确匹配甚至外推未见过的更高回报，展现多任务分布建模能力。**
[PIC]

#### 5.3 长上下文是关键
消融 K（上下文长度）。
结论：**K=1 时性能骤降；K≥30 的完整轨迹让模型识别“哪条策略产生该动作”，显著提升效果。**
[PIC]

#### 5.4 长期信用分配有效
Key-to-Door 稀疏奖励任务：只有首尾两步有关键奖励。
结论：**DT 成功率 94.6%，而 CQL 仅 13%，Transformer 自注意力直接跨越中间噪声步完成信用分配。**
不过这个只在一个环境上进行比较
[PIC]

#### 5.5 **Transformer 也能当 critic**
让模型同时预测动作与初始回报。
结论：**注意力头聚焦“拿钥匙”和“到门”关键步，模型内部已学到状态-奖励关联，可做准确值估计。**

```python
        ...
        x = tf_out['last_hidden_state']
        
        # reshape x so that the second dimension corresponds to the original 
        # returns (0), states (1), or actions (2); i.e. x[:, 1, t] is token for s_t
        x = x.reshape(batch_size, seq_length, 3, self.hidden_size).permute(0, 2, 1, 3)
        
        # get predictions
        return_preds = self.predict_return(x[:,2])  # predict next return given state and action
        state_preds = self.predict_state(x[:,2])    # predict next state given state and action
        action_preds = self.predict_action(x[:,1])  # predict next action given state

        return state_preds, action_preds, return_preds
```
[PIC]


#### 5.6 稀疏/延迟奖励鲁棒

这个在 [ICLR 2024: WHEN SHOULD WE PREFER DECISION TRANSFORMERS FOR OFFLINE REINFORCEMENT LEARNING](https://arxiv.org/pdf/2305.14550) 有详细探讨
把 D4RL 的逐步奖励改成只在最后一步给总和。
结论：**CQL 崩溃（<5 分），DT 几乎无损；序列建模对奖励密度无依赖。**
[PIC]


#### 5.7 无需保守/正则
解释为何 DT 不用 value pessimism。
结论：传统 TD 方法用近似值函数做优化目标，误差会被放大；DT 直接生成动作，不优化学习来的函数，自然避免外推误差。


#### 5.8 对在线 RL 的启示
结论：DT 可作为“记忆引擎”+ 探索算法（如 Go-Explore）组合，把离线学到的先验迁移到在线探索，实现样本高效在线 RL。

### 7- 总结

> Decision Transformer 首次证明“用 GPT 做 RL”就能在离线任务上击败传统方法；未来只需在自监督预训练、随机回报建模、模型偏差与数据安全上继续深耕，即可把这套“RL 即生成”范式推向大规模落地。

我们提出了 Decision Transformer，旨在把语言/序列建模的思想与强化学习统一起来。
在标准离线 RL 基准上，我们证明了 Decision Transformer 仅需对标准语言模型架构做极少改动，即可媲美甚至超越专门为离线 RL 设计的强算法。
我们希望这项工作能激发更多关于在 RL 中使用大型 Transformer 的研究。
实验中我们采用了一条简单的有监督损失就取得了良好效果，但如果面向大规模数据集，或许可从自监督预训练任务中进一步获益。
此外，还可以为回报、状态和动作设计更精细的嵌入方式——例如，用回报分布而非确定性回报来做条件，以刻画随机场景。
Transformer 也可以被用来建模轨迹的状态演化，从而可能替代基于模型的 RL；我们希望在未来的工作中对此进行探索。
对于现实部署，很重要的一点是要理解 Transformer 在 MDP 场景下会犯何种错误以及可能带来的负面后果，这一点目前研究甚少。
我们还需关注训练所用的数据集本身，因为它们可能引入破坏性偏差；尤其是当我们考虑用更多来源可疑的数据来增强 RL 智能体时，这一点尤为关键。
例如，恶意行为者设计的奖励信号可能导致模型在按期望回报生成行为时产生非预期的行为。


### OTH-DecisionTransformerModel VS GPT2Model

| 维度    | GPT2Model                                     | DecisionTransformerModel          |
| ----- | --------------------------------------------- | --------------------------------- |
| 网络结构  | 原生 GPT-2 Decoder（Masked Self-Attention + FFN） | 复用 **完全相同** 的 GPT-2 Decoder 层     |
| 层数/宽度 | 默认 12 层 768 隐维                                | 继承同一权重，但可通过 `config.n_layer` 随意改  |
| 位置编码  | 学习式绝对位置       | 忽略，全部设置为0 |
| 序列格式     | `[token1, token2, …]` | `[r1, s1, a1, r2, s2, a2, …]` <br>每步 3 个 token（return-to-go, state, action） |
| 嵌入层      | 仅 Token + Pos         | 额外 **ReturnEmbeddings + StateEmbeddings + ActionEmbeddings**                |
| 损失 | 语言建模交叉熵：预测下一个 token | 回归 MSE：仅预测 **action token** |
| 标签 | 输入右移 1 位            | 输入保持不变，仅对 $a_t$ 位置计算损失       |


#### 本项目中的实现

项目路径构成
```
.
├── RLAlgo
...
│   ├── batchRL
│   │   ├── cql.py
│   │   ├── decision_tf.py
│   │   ├── DTModules
│   │   │   ├── DT_model.py
│   │   │   └── trajectory_gpt2.py
....
├── RLUtils
│   ├── batchRL
│   │   ├── trainer.py
│   │   └── utils.py
...
```

- `RLAlgo.batchRL.DTModules` 基本参考的[kzl: gym.decision_transformer](https://github.com/kzl/decision-transformer/blob/master/gym/decision_transformer/models)
  - 参考`transformers.DecisionTransformerModel`调整 `DT_model` 
    - 新增Line=66  `config.n_ctx = max_ep_len`, 
    - 新增Line=126 `position_ids=torch.zeros(stacked_attention_mask.shape, device=stacked_inputs.device, dtype=torch.long),`
- `RLAlgo.batchRL.DTModules.decision_tf`, 参考 [kzl: gym.decision_transformer.training.seq_trainer](https://github.com/kzl/decision-transformer/blob/master/gym/decision_transformer/training/seq_trainer.py)
    - 框架调整，适配 `RLUtils.batchRL.trainer`
- `RLUtils.batchRL.trainer`和`RLUtils.batchRL.utils` 参考 [kzl: gym.experiment](https://github.com/kzl/decision-transformer/blob/master/gym/experiment.py)
  - 将 `experiment.py` 中的`get_batch` 调整： `RLUtils.batchRL.utils.rtgEpDataset` 直接适配`trainer.DT_training`中的`dloader = DataLoader(rtgEpDataset(episode_data, K=cfg.K), batch_size=cfg.batch_size, shuffle=True)`
  - 将 `experiment.py` 中的`evaluate_episode_rtg` 调整：`RLUtils.batchRL.trainer.dt_play`

