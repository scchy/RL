## Reinforcement learning is direct adaptive optimal control

paper Link: [Reinforcement learning is direct adaptive optimal control](http://incompleteideas.net/papers/sutton-barto-williams-91.pdf)

这篇论文是早期将 RL 与控制理论结合的里程碑，为后续研究（如近似动态规划、Actor-Critic 方法）奠定了基础。


核心观点总结:
1. 统一视角：论文将强化学习（RL）视为一种直接自适应最优控制方法。也就是说，RL 算法通过与环境交互，直接学习一个最优策略，而不需要事先知道环境的完整动力学模型。
2. 与传统控制的对比：  
   1. 经典最优控制（如 LQR、DP）通常需要精确的系统模型。
   2. RL 通过试错和奖励信号，在线调整策略，适应未知或变化的环境。
3. 关键方法：
   1. 使用时间差分（TD）学习和策略梯度等方法，逼近最优值函数或策略。
   2. 强调**增量式、在线学习**的能力，适合高维、复杂系统


### 1. 摘要

本文从控制系统的角度介绍了学习控制强化学习的主要神经网络方法之一。 控制论问题可以拆分成2大类：
1. 调节和跟踪问题，其目标是遵循参考轨迹
2. 最优控制问题，其目标是使受控系统行为的一个函数极值，该函数不一定用参考轨迹来定义

已经研究的自适应最优控制方法几乎都是间接方法，其中在每一步都从估计的系统模型中重新计算控制。计算本身就很复杂，这使得直接估计最优控制的自适应方法更具吸引力。我们认为强化学习方法是一种计算简单、直接的非线性系统自适应最优控制方法。为了具体起见，我们关注一种强化学习方法（Q-learning）及其对一类自适应最优控制问题的分析证明能力

### 2. 简介

所有控制问题都涉及操纵动态系统的输入，使其行为符合构成控制目标的一系列规范。
1. 在一些问题中，控制目标是根据受控系统的输出阈值尽可能接近或跟踪的参考水平或参考轨迹来定义的。
   1. 关键问题：稳定性
2. 在其他问题中，控制目标是使受控系统行为的一个函数极值，该函数不一定用参考水平或轨迹来定义
   1. 关键问题：约束优化


   基于变分法（calculus of variations）和动态规划(dynamic programming)的最优控制方法在这里得到了广泛的研究。
近年来，最优控制受到的关注不如调节和跟踪，调节和跟踪已被证明在分析和计算上都更易于处理，并且在许多应用中产生了更可靠的控制。
   当无法获得待控制系统的详细准确模型时，可以应用自适应控制方法。**自适应控制方法的压倒性优势解决了调节和跟踪问题**。
然而，如果能够开发出计算上可行且可以稳健地应用于**非线性系统的方法**，那么用于最优控制问题的**自适应方法将得到广泛应用**。

   跟踪问题假设对参考轨迹有先验知识，但对于许多问题来说，确定参考轨迹是整个问题的重要组成部分，如果不是最重要的部分的话。

例如，轨迹规划是机器人控制任务中的一个关键和难点问题。为了设计一个能够两足行走的机器人，人们可能无法先验地为四肢指定所需的轨迹，但可以**指定前进、保持平衡、不损坏机器人等目标**。

   过程控制任务通常根据总体目标来指定，例如最大限度地提高产量或最小化能耗。
通常不可能通过将任务划分为单独的阶段进行轨迹规划和轨迹跟踪来实现这些目标。理想情况下，我们希望确定轨迹和所需的控制，以使目标函数最大化。

对于跟踪和最优控制，通常区分间接和直接自适应控制方法。
1. 间接方法：依赖于系统识别过程来形成受控系统的显式模型，然后确定控制规则形式模型
2. 直接方法：在不形成这样的系统模型的情况下重新确定控制规则

论文中，作者简要介绍了被称为RL方法的学习方法，然后将其作为一种直接的自适应最优控制方法。这些方法源于动物学习研究和早期学习控制工作, 现在是神经网络和机器学习领域的一个活跃研究领域。我们在这里总结了通过将这些方法视为动态规划和随机近似方法的综合而获得的对这些方法的更深入的理解


### 3. RL 

**RL基于这样一种共识，即如果一个行动之后出现了令人满意的事态，或者事态有所改善（以某种明确界定的方式确定），
那么产生该行动的趋势就会增强，即强化.**

这一思想在动物学习理论、参数摄动自适应控制方法以及学习自动机和多臂老虎机问题理论中起着基础作用。
将这一想法扩展到允许动作选择依赖于状态信息，引入了反馈控制、模式识别和联想学习的各个方面。
此外，可以将“随后是令人满意的事态”的概念扩展到包括行动的长期后果。
通过将调整行动选择的方法与估计行动长期后果的方法相结合，可以设计出可用于控制涉及时间扩展行为的问题的rl方法

我们得到的大多数形式化结果都是关于具有未知转移概率的马尔可夫过程的控制。
**同样重要的是，形式化结果表明，与传统方法相比，可以使用更异步或增量形式的动态规划来计算最优控制。**
使用RL与神经网络或其他联想记忆结构相结合的经验（模拟）结果表明，在各种非线性控制问题上具有鲁棒的高效学习能力

RL神经网络在非线性控制问题中的研究通常集中在两种主要算法之一：
1. actor-critic learning : critic-预估状态的长期价值； actor-选择动作
2. Ql-earing: 预估所有状态-动作对的价值，基于预估选择动作

这些技术中的任何一种都可以作为直接自适应最优控制算法的一个例子，但由于Q-学习在概念上更简单，理论更发达，
并且在许多情况下被经验发现收敛更快，我们在这里详细介绍了这种特殊的技术，并省略了对行动者批评学习的进一步讨论

### 4. Q-Learning 

> 在有限状态和有限动作的空间里

$E [ \sum_{j=0}^\infin \gamma^j r_{k+j} ]; \gamma \in [0, 1)$

- 当前到后续的长期价值

迭代
$Q(x, a)=E\{r_k + \gamma \max_b Q(x_k+1, b) | x_k=x, a_k=a \}$

$\hat{Q}(x_k, a_k) += \beta_k [r_k + \gamma \max_b \hat{Q}(x_k+1, b) - \hat{Q}(x_k, a_k) ]; \beta_k \in (0, 1)$

action需要增加一定的随机性探索来帮助收敛。

Q函数，另一方面来说，结合状态转移信息和未来价值，不依赖状态转移概率的显式估计。


### 5. 混合直接/间接方法

与传统控制的对比：  
   1. 经典最优控制（如 LQR、DP）通常需要精确的系统模型。
   2. RL 通过试错和奖励信号，在线调整策略，适应未知或变化的环境。

Dyna:
1) performs RL using actual experiences
2) applies the same rl method to model-generated experiences
3) updates the system model based on actual experiences

简单且高效的方法来结合学习和增量规划能力。

### 6. 总结

当从系统模型设计控制的过程在计算上很简单时，就像在线性调节和跟踪任务中一样，间接和直接自适应方法之间的区别对自适应控制方法的可行性影响很小。
然而，当设计过程非常昂贵时，如非线性最优控制，间接方法和直接方法之间的区别变得更加重要。
在本文中，我们提出了RL作为一种在线DP方法和一种计算成本低廉的**直接自适应最优控制**方法。
这种方法有助于整合动物学习、人工智能的见解，也许——正如我们所说的——还有最优控制理论。



