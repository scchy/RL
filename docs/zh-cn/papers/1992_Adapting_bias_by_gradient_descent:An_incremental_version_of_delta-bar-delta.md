## IDBD: Adapting bias by gradient descent: An incremental version of delta-bar-delta

paper Link: [IDBD: Adapting bias by gradient descent: An incremental version of delta-bar-delta](http://incompleteideas.net/papers/sutton-92a.pdf)
 
### 1. 简介

这篇论文《Adapting bias by gradient descent: An incremental version of delta-bar-delta》由 Richard S. Sutton 在 1992 年发表，提出了一种新的算法——增量 Delta-Bar-Delta (IDBD) 算法，用于基于以往学习经验学习适当的偏差（bias）。以下是对论文内容的总结：

### 论文背景
- **学习与泛化的关键**：适当的偏差被认为是高效学习和泛化的关键。人类能够快速学习并准确泛化，是因为他们在学习情境中带有一组适当的偏差，这些偏差引导他们偏好某些假设而非其他。
- **偏差的来源**：机器学习系统若要达到人类的表现水平，也需要一套适当的偏差。然而，偏差通常是由人类设计的，而非自动获取。作者认为，偏差应从以往的学习经验中生成，这要求学习者遇到一系列需要相同或相似偏差的不同问题。

### IDBD 算法 (类似优化器)
- **算法概述**：IDBD 算法是一种元学习算法，用于学习一个简单线性学习系统（LMS 或 delta 规则）的学习率参数。这些学习率参数是该系统的一种重要偏差形式。IDBD 算法基于以往学习经验来调整学习率参数。
- **算法细节**：
    - 基础学习系统是一个单一线性单元，其输出是输入的加权和($y_t=W_t^TX_t$)，学习目标是最小化未来时间步上的平方误差$w_i(t+1) = w_i(t) + \alpha \delta x_i(t)$。
    - 在 IDBD 算法中，每个输入都有一个不同的学习率，这些学习率通过一个元学习过程进行调整。
    - 学习率的形式为 $\alpha_u(t) = e^{\beta _i(t)}$，其中 $\beta _i$是实际被修改的记忆参数。这种指数关系保证了学习率始终为正，并且可以实现学习率的几何步长变化。
    - $\beta _i$ 的更新公式为 $\beta _i(t+1) = \beta _i(t) + \theta \delta(t)x_i(t)h_i(t)$ ，其中 θ 是元学习率，$h_i(t)$是每个输入的额外记忆参数，其更新公式为 $h_i(t+1) =h_i(t)[1-\alpha_i(t+1)x_i^2(t)]^{+} + \alpha_i(t+1) \delta(t)x_i(t)$ 
    - 算法的核心思想是: <b><font color=darkred>通过累积当前权重变化与最近权重变化之间的乘积，使 βi 的总变化与当前和最近权重变化之间的相关性成比例。如果当前步骤与过去的步骤正相关，则表明过去的步骤应该更大；如果当前步骤与过去的步骤负相关，则表明过去的步骤过大，算法在最佳权重值上过度调整，然后不得不在相反方向上重新调整。</font></b>
    - 与 Jacobs 的 Delta-Bar-Delta 算法相比，IDBD 算法是增量的，适用于逐个处理示例的监督学习任务，并且只有一个自由参数，即元学习率 θ。

### 实验
- **实验 1**：验证 IDBD 算法是否比普通 LMS 算法表现更好。任务涉及 20 个实值输入和一个输出，输入按正态分布随机选择，目标概念是前五个输入的加权和，权重为 +1 或 -1，并且每 20 个示例随机改变一个权重的符号。实验结果表明，IDBD 算法在渐近跟踪性能上明显优于普通 LMS 算法，平均均方误差约为 1.5，而普通 LMS 算法的最佳均方误差约为 3.5。
- **实验 2**：验证 IDBD 算法是否能找到最优的学习率分布。实验中选择了较小的元学习率 θ = 0.001，并运行了大量示例（250,000）以观察算法找到的学习率的渐近分布。结果显示，对于相关输入，学习率稳定在 0.13±0.015，而对于不相关输入，学习率趋向于零。通过固定相关输入的学习率为不同值并测量平均平方误差，确认了 IDBD 算法找到的学习率接近最优值。

### 算法推导
- **梯度下降推导**：IDBD 算法可以被推导为学习率参数 βi 的梯度下降形式。通过类似 LMS 规则的梯度下降分析，可以得到 βi 和 hi 的更新公式。这种推导表明，IDBD 算法是一种随机梯度下降算法，会根据其对整体误差的影响来调整学习率参数。

### 结论
- **IDBD 算法的能力**：IDBD 算法能够区分相关和不相关的输入，并在增量跟踪任务中找到最优的学习率。这种找到适当偏差的能力可以显著降低误差，在本文的任务中，平方误差降低了约 60%。
- **算法优势**：IDBD 算法在保持线性增加的内存和计算量的同时，实现了这一能力，且比普通 LMS 算法多出的计算量大约是一个因素三。该算法是增量的，基于逐个示例进行操作，并且自由参数更少。
- **进一步研究**：虽然本文**只探索了 IDBD 算法的线性版本**，但梯度分析可能对增量偏差学习算法的一般情况有所启示，例如可以尝试为基于实例的学习方法推导偏差学习算法。此外，IDBD 算法还在其他方面进行探索，如作为心理学模型，评估构造归纳方法或其他表示变化方法创建的特征的效用等。

### 总结
这篇论文提出了一种新的算法——IDBD 算法，用于基于以往学习经验学习适当的偏差。该算法在非平稳跟踪任务中表现出色，能够显著降低误差，并且具有线性增加的内存和计算需求。论文还提供了算法的梯度下降推导，为进一步研究提供了基础。