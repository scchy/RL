
# ä¸€ã€RAGä¸»è¦æµç¨‹

```mermaid
graph LR

A1(query) -->|Recieveæ–‡æ¡£| B1(æ£€ç´¢ç‰‡æ®µ)-->B2(åŸºäºæ£€ç´¢ç”ŸæˆPrompt-construct_prompt)-->|ç”Ÿæˆ|C1(LLM generateå›ç­”)-->|send|D1(Respond)

B1-->BStep1(æ–‡æ¡£æ‹†åˆ†æˆchunks)-->BStep2(æ–‡æ¡£chunksç”¨LLM-embedding)-->BStep3(ä¿å­˜chunks embedding)-->B2

```

æ³¨ï¼š
- generateå’Œembeddingå¯ä»¥ä¸åŒæ¨¡å‹
  - LLM-embedding: æ¨èä½¿ç”¨ æ™ºæºçš„BGE  `BAAI/bge-en-icl 	$0.0100/Mtoken`
    -  In-context Learning (ICL) : é€šè¿‡æä¾›å°‘é‡ç¤ºä¾‹ï¼ˆfew-shot examplesï¼‰æ¥æ˜¾è‘—æå‡æ¨¡å‹å¤„ç†æ–°ä»»åŠ¡çš„èƒ½åŠ›
    -  ä¹Ÿå¯ä»¥ç”¨é˜¿é‡Œäº‘çš„ `text-embedding-v3`
  - LLM generateï¼šå¯ä»¥ç›´æ¥ç”¨ `deepseek-chat`
- ä¿å­˜chunks embedding
  - å¯ä»¥ç”¨ç®€å•çš„dict
  - å¯ä»¥ç”¨å‘é‡æ•°æ®åº“ `chromadb` ç­‰


# äºŒã€æœ¬åœ°ä¿å­˜çš„å…¨æµç¨‹å®ç°

## 2.1 æ–‡æœ¬ä¿å­˜æˆEmbedding chunks

```mermaid
graph LR
Step1(æ–‡æ¡£è¯»å–) --> Step2(æ–‡æ¡£æ‹†åˆ†æˆchunks)--> Step3(æ–‡æ¡£chunksç”¨LLM-embedding)--> Step4(ä¿å­˜chunks embedding)
```

### 2.1.1 æ–‡æ¡£è¯»å–&æ‹†åˆ†æˆchunkâ€”â€”ä»¥textæ–‡æ¡£ä¸ºä¾‹

> pdfæ–‡æ¡£è§£æå¯ä»¥çœ‹è¿™ä¸ªæ–‡æ¡£ï¼š https://www.aneasystone.com/archives/2025/03/pdf-parser-libraries.html

1. `load_documents(dir_path: str) -> List[str]`: è¯»å–æ–‡ä»¶ä¸‹æ‰€æœ‰çš„txtæ–‡ä»¶
2. `split_into_clean_chunks(docs: List[str], chunk_size: int = 30) -> List[str]`: å°†è¯»å–çš„æ–‡ä»¶å†…å®¹ï¼Œæ‹†åˆ†ä¸ºå¤§å°ä¸ºchunk_sizeçš„chunkåˆ—è¡¨

```python
def load_documents(dir_path: str) -> List[str]:
    docs = []
    for f in os.listdir(dir_path):
        if f.endswith('.txt'):
            with open(os.path.join(dir_path, f), 'r', encoding='utf-8') as f_o:
                docs.append(f_o.read())
    return docs


def preprocess_text(text: str) -> str:
    text = text.lower()
    # Remove special characters, keeping only alphanumeric characters and spaces
    text = ''.join(char for char in text if char.isalnum() or char.isspace())
    return text


def split_into_clean_chunks(docs: List[str], chunk_size: int = 30) -> List[str]:
    chunks = []  
    for doc in docs:  
        words = doc.split()  
        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i + chunk_size])   
            chunks.append(preprocess_text(chunk))
    return chunks 


def prepare_text_test():
    cur_p = os.path.dirname(__file__)
    dir_p = os.path.join(cur_p, "data")
    documents = load_documents(dir_p)
    preprocessed_chunks = split_into_clean_chunks(documents)
    for i in range(2):
        print(f"Chunk {i+1}: {preprocessed_chunks[i][:50]} ... ")
        print("-" * 50)  
```

### 2.1.2 embedding & save

1. åˆ†æ‰¹ç”ŸæˆEmbeddingï¼š `generate_embeddings(chunks: List[str], batch_size: int = 10) -> np.ndarray`  
   1. æ¯batch_sizeå¤§å°chunksï¼Œ è°ƒç”¨ä¸€æ¬¡api `BAAI/bge-en-icl`
   2. `generate_embeddings_batch`è°ƒç”¨
2. ä¿å­˜æˆç®€å•çš„Dictï¼š `add_to_vector_store(embeddings: np.ndarray, chunks: List[str])`  
   1. ä¿å­˜æˆï¼š`{0:{"embedding": "np.arrray", "chunk": "string"}}`


```python
def generate_embeddings_batch(chunks_batch: List[str], model: str = "BAAI/bge-en-icl") -> List[List[float]]:
    """
    # BAAI/bge-en-icl 	$0.0100/Mtoken  
    """
    response = emb_client.embeddings.create(
        model=model,   
        input=chunks_batch,
        encoding_format='float'
    )
    embeddings = [item.embedding for item in response.data]
    return embeddings


def generate_embeddings(chunks: List[str], batch_size: int = 10) -> np.ndarray:
    all_embeddings = []
    for i in tqdm(range(0, len(chunks), batch_size)):
        batch = chunks[i:i + batch_size]
        embeddings = generate_embeddings_batch(batch)
        all_embeddings.extend(embeddings)
    return np.array(all_embeddings)


vector_store: dict[int, dict[str, object]] = {}
def add_to_vector_store(embeddings: np.ndarray, chunks: List[str]) -> None:
    for embedding, chunk in zip(embeddings, chunks):
        vector_store[len(vector_store)] = {"embedding": embedding, "chunk": chunk}

```

# ä¸‰ã€ç®€å•RAGæµç¨‹

```mermaid
graph LR
Step1(æ£€ç´¢ç›¸å…³ç‰‡æ®µ) --> Step2(æ„å»ºæç¤º)-->|ç”Ÿæˆ|Step3(LLM generateå›ç­”)-->|send|D1(Respond)
```


## 3.1 æ£€ç´¢ç›¸å…³ç‰‡æ®µ

1. queryè½¬æˆembedding: `generate_embeddings([query_text])[0]`
2. åŸºäºqueryå’Œä¿å­˜çš„å‘é‡æ–‡æ¡£`vector_store`ï¼Œæœç´¢æœ€ç›¸å…³çš„TopNä¸ªæ–‡æ¡£ç¢ç‰‡
   1. `similarity_search(query_embedding: np.ndarray, top_k: int = 5)`


```python
def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    dot_product = np.dot(vec1, vec2)
    return dot_product / (np.linalg.norm(vec1) * np.linalg.norm(vec2))


def similarity_search(query_embedding: np.ndarray, top_k: int = 5) -> List[str]:
    similarities = []
    for key, value in vector_store.items():
        similarity = cosine_similarity(query_embedding, value["embedding"])
        similarities.append((key, similarity))

    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)
    # å€’åºæ’åºå–Top N
    return [vector_store[key]["chunk"] for key, _ in similarities[:top_k]]


def retrieve_relevant_chunks(query_text: str, top_k: int = 5) -> List[str]:
    query_embedding = generate_embeddings([query_text])[0]
    relevant_chunks = similarity_search(query_embedding, top_k=top_k)
    return relevant_chunks

```

## 3.2 æ„å»ºæç¤º

1. System: æŒ‡å‡ºæ ¹æ®æä¾›ä¿¡æ¯å›ç­”é—®é¢˜
2. Context: æ”¾å…¥æ£€ç´¢å‡ºæ¥çš„æ–‡æ¡£ç‰‡
3. Question: æé—®çš„æ–‡æœ¬

```python
def construct_prompt(query: str, context_chunks: List[str]) -> str:
    """
    é€šè¿‡å°†æŸ¥è¯¢ä¸æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µç»“åˆï¼Œæ„å»ºæç¤ºã€‚
    å‚æ•°ï¼š
        query (str): è¦æ„å»ºæç¤ºçš„æŸ¥è¯¢æ–‡æœ¬ã€‚
        context_chunks (List[str]): è¦åŒ…å«åœ¨æç¤ºä¸­çš„ç›¸å…³ä¸Šä¸‹æ–‡ç‰‡æ®µåˆ—è¡¨ã€‚

    è¿”å›ï¼š
        str: ç”¨äºä½œä¸º LLM è¾“å…¥çš„æ„å»ºå¥½çš„æç¤ºã€‚
    """
    # chinese_prompt_template = """
    # System:
    # ä½ æ˜¯ä¸€ä¸ªé—®ç­”æœºå™¨äººã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹è¿°ç»™å®šçš„å·²çŸ¥ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    # å¦‚æœå·²çŸ¥ä¿¡æ¯ä¸åŒ…å«ç”¨æˆ·é—®é¢˜çš„ç­”æ¡ˆï¼Œæˆ–è€…å·²çŸ¥ä¿¡æ¯ä¸è¶³ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·ç›´æ¥å›å¤"æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜"ã€‚

    # å·²çŸ¥ä¿¡æ¯:
    # {context} # æ£€ç´¢å‡ºæ¥çš„åŸå§‹æ–‡æ¡£

    # ç”¨æˆ·é—®é¢˜:
    # {query} # ç”¨æˆ·çš„æé—®

    # å›ç­”:
    # """
    context = "\n".join(context_chunks)
    system_message = (
        "You are a helpful assistant. Only use the provided context to answer the question. "
        "If the context doesn't contain the information needed, say 'I don't have enough information to answer this question.'"
    )
    prompt = f"System: {system_message}\n\nContext:\n{context}\n\nQuestion:\n{query}\n\nAnswer:"
    return prompt

```

## 3.3 ç”Ÿæˆç­”æ¡ˆ


```python
def generate_response(
    prompt: str,
    model: str = "deepseek-chat",
    client_in = None,
    max_tokens: int = 512,
    temperature: float = 1,
    top_p: float = 0.9,
    top_k: int = 50
) -> str:
    """
    æ ¹æ®æ„å»ºçš„promptä»OpenAI-æ¨¡å‹ç”Ÿæˆå›ç­”

    Args:
        prompt (str): construct_prompt ç”Ÿæˆçš„æç¤ºè¯
        model (str): LLM default "deepseek-chat" "google/gemma-2-2b-it å›½å†…æ— æ³•æ­£å¸¸ä½¿ç”¨".
        max_tokens (int): ç”Ÿæˆå›ç­”çš„æœ€å¤štokensæ•°  Default is 512.
        temperature (float): Sampling temperature for response diversity. Default is 0.5.
        top_p (float): Probability mass for nucleus sampling. Default is 0.9.
        top_k (int): Number of highest probability tokens to consider. Default is 50.

    Returns:
        str: The generated response from the chat model.
    """
    client = client_in if client_in is not None else client
    response = client.chat.completions.create(
        model=model,   
        max_tokens=max_tokens,   
        temperature=temperature,   
        top_p=top_p,   
        extra_body={   
            "top_k": top_k  
        },
        messages=[   
            {
                "role": "user", 
                "content": [  
                    {
                        "type": "text",  # Type of content (text in this case)
                        "text": prompt  # The actual prompt text
                    }
                ]
            }
        ]
    )
    # Return the content of the first choice in the response
    return response.choices[0].message.content


def basic_rag_pipeline(query: str, model: str="deepseek-chat", api_client=None) -> str:
    """
    å®ç°åŸºç¡€æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG) pipeline
    æ£€ç´¢ç›¸å…³ç‰‡æ®µ -> æ„å»ºæç¤º -> å¹¶ç”Ÿæˆå›ç­”
    Args:
        query (str): è¾“å…¥æŸ¥è¯¢ï¼Œç”¨äºç”Ÿæˆå›ç­”ã€‚
    Returns:
        str: åŸºäºæŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Œç”± LLM ç”Ÿæˆçš„å›ç­”ã€‚
    """
    relevant_chunks = retrieve_relevant_chunks(query)
    prompt = construct_prompt(query, relevant_chunks)
    response = generate_response(prompt, model=model, client_in=api_client)
    return response
```

# å››ã€æµ‹è¯•

```python

def RAG_test():
    cur_p = os.path.dirname(__file__)
    dir_p = os.path.join(cur_p, "data")
    documents = load_documents(dir_p)
    preprocessed_chunks = split_into_clean_chunks(documents)
    emb_f = os.path.join(cur_p, "data", "embeddings.json")
    embeddings = load_embedding(emb_f)
    add_to_vector_store(embeddings, preprocessed_chunks)

    test_f = os.path.join(cur_p, "data", 'val.json')
    with open(test_f, 'r') as file:
        validation_data = json.load(file)

    sample_query = validation_data['basic_factual_questions'][0]['question']  
    expected_answer = validation_data['basic_factual_questions'][0]['answer']  

    print(f"Sample Query: {sample_query}\n")
    print(f"Expected Answer: {expected_answer}\n")

    print("ğŸ” Running the Retrieval-Augmented Generation (RAG) pipeline...")
    print(f"ğŸ“¥ Query: {sample_query}\n")

    # Run the RAG pipeline and get the response
    #  $0.02/$0.04 in/out Mtoken  google/gemma-3-4b-it
    # response = basic_rag_pipeline(sample_query, model='google/gemma-3-4b-it', api_client=emb_client)
    response = basic_rag_pipeline(sample_query, model='deepseek-chat', api_client=client)
    # Print the response with better formatting
    print("ğŸ¤– AI Response:")
    print("-" * 50)
    print(response.strip())
    print("-" * 50)

    # Print the ground truth answer for comparison
    print("âœ… Ground Truth Answer:")
    print("-" * 50)
    print(expected_answer)
    print("-" * 50)
    response_embedding = generate_embeddings([response])[0]
    ground_truth_embedding = generate_embeddings([expected_answer])[0]
    similarity = cosine_similarity(response_embedding, ground_truth_embedding)
    
    print(f"âœ… similarity: {similarity:.5f}")
```

è¾“å‡ºå¦‚ä¸‹:
```
Sample Query: What is the mathematical representation of a qubit in superposition?
Expected Answer: |ÏˆâŸ© = Î±|0âŸ© + Î²|1âŸ©, where Î± and Î² are complex numbers satisfying |Î±|Â² + |Î²|Â² = 1, representing the probability amplitudes for measuring the qubit in state |0âŸ© or |1âŸ© respectively.

ğŸ” Running the Retrieval-Augmented Generation (RAG) pipeline...
ğŸ“¥ Query: What is the mathematical representation of a qubit in superposition?


ğŸ¤– AI Response:
--------------------------------------------------
The mathematical representation of a qubit in superposition is given by:  
**Ïˆ = Î±|0âŸ© + Î²|1âŸ©**,  
where Î± and Î² are complex numbers satisfying |Î±|Â² + |Î²|Â² = 1. These represent the probability amplitudes for measuring the qubit in state |0âŸ© or |1âŸ©, respectively.  

(Answer derived directly from the provided context.)
--------------------------------------------------
âœ… Ground Truth Answer:
--------------------------------------------------
|ÏˆâŸ© = Î±|0âŸ© + Î²|1âŸ©, where Î± and Î² are complex numbers satisfying |Î±|Â² + |Î²|Â² = 1, representing the probability amplitudes for measuring the qubit in state |0âŸ© or |1âŸ© respectively.
--------------------------------------------------
âœ… similarity: 0.92927
```

# äº”ã€å…³äºapi 

1. deepseek: 
   1. `client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")`
   2. å®˜ç½‘ï¼š[deepseek](https://api-docs.deepseek.com/zh-cn/quick_start)
   3. æ¨¡å‹ï¼š`deepseek-chat`: 2å…ƒ/8å…ƒ  in/out Mtoken | 0.5å…ƒ   è¾“å…¥ç¼“å­˜å‘½ä¸­
2. deepinfra
   1. `emb_client = OpenAI(api_key=df_api_key, base_url="https://api.deepinfra.com/v1/openai")`
   2. å®˜ç½‘ï¼š[deepinfra](https://deepinfra.com)
   3. æ¨¡å‹ï¼š`BAAI/bge-en-icl`: $0.0100/Mtoken ;  `google/gemma-3-4b-it`: $0.02/$0.04 in/out Mtoken
3. é˜¿é‡Œäº‘ç™¾ç‚¼
   1. `client = OpenAI(api_key=os.getenv("DASHSCOPE_API_KEY"), base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")`
   2. å®˜ç½‘ï¼š[é˜¿é‡Œäº‘ç™¾ç‚¼](https://bailian.console.aliyun.com/)
   3. æ¨¡å‹ï¼š`qwen-max`, `qwen-plus`, `qwen-turbo`, `qwen-long` [é€šä¹‰åƒé—®æ¨¡å‹åˆ—è¡¨](https://help.aliyun.com/zh/model-studio/models)



# å…­ã€ä¿å­˜åˆ°å‘é‡æ•°æ®åº“çš„RAG

1. å’Œç®€å•å®ç°çš„å·®å¼‚
   1. åˆ†æ‰¹ç”ŸæˆEmbeddingï¼š `generate_embeddings -> simpleVectorDB.batch_add_documents()`  
      1. è¿˜æ˜¯è°ƒç”¨`generate_embeddings_batch`è°ƒç”¨
      2. batch_add_documents ä¸­ç›´æ¥ç”¨ collection.add ä¿å­˜åˆ°äº†å‘é‡æ•°æ®åº“ä¸­
         1. ä¸ç”¨å†æ‰§è¡Œç®€å•å®ç°ä¸­çš„`add_to_vector_store`
   2. æœç´¢æœ€ç›¸å…³çš„TopNä¸ªæ–‡æ¡£ç¢ç‰‡

```python

import chromadb
from chromadb.config import Settings
from functools import partial


class simpleVectorDB:
    def __init__(self, collection_name, embedding_fn=generate_embeddings_batch):
        self.collection_name = collection_name
        self.chroma_client = chromadb.Client(Settings(allow_reset=True))
        self.collection = self.chroma_client.get_or_create_collection(name=collection_name)
        self.embedding_fn = embedding_fn
        self.add_counts = 0

    def add_documents(self, documents):
        '''å‘ collection ä¸­æ·»åŠ æ–‡æ¡£ä¸å‘é‡'''
        # print(f'self.add_counts={self.add_counts}', documents[:2])
        self.collection.add(
            embeddings=self.embedding_fn(documents),
            documents=documents,
            ids=[f"id{self.add_counts}_{i}" for i in range(len(documents))]
        )
        self.add_counts += 1

    def reset(self):
        self.chroma_client.reset()
        self.collection = self.chroma_client.get_or_create_collection(name=self.collection_name)
        self.add_counts = 0

    def search(self, query: str, top_n: int=5):
        '''æ£€ç´¢å‘é‡æ•°æ®åº“'''
        results = self.collection.query(
            query_embeddings=self.embedding_fn([query]),
            n_results=top_n
        )
        return results['documents'][0]

    def batch_add_documents(self, chunks: List[str], batch_size: int = 10):
        all_embeddings = []
        for i in tqdm(range(0, len(chunks), batch_size)):
            self.add_documents(chunks[i:i + batch_size])


def vdb_search_test():
    cur_p = os.path.dirname(__file__)
    dir_p = os.path.join(cur_p, "data")
    documents = load_documents(dir_p)
    preprocessed_chunks = split_into_clean_chunks(documents)
    v_db = simpleVectorDB(
        'ragVectorDB-tt1', 
        partial(generate_embeddings_batch, model='text-embedding-v3', emb_client=ali_client)
    )
    v_db.reset()
    v_db.batch_add_documents(preprocessed_chunks)
    query_text = "What is Quantum Computing?"
    relevant_chunks = v_db.search(query_text)
    for idx, chunk in enumerate(relevant_chunks):
        print(f"Chunk {idx + 1}: {chunk[:50]} ... ")
        print("-" * 50)  # Print a separator line
```

## 6.1 å®Œæ•´pipelineç¤ºä¾‹


```python

class VDB_RAG_Bot:
    def __init__(
        self, 
        collection_name: str='ragVectorDB', 
        client: OpenAI = ali_client,
        embedding_fn=partial(generate_embeddings_batch, model='text-embedding-v3', emb_client=ali_client)
    ):
        self.v_db = simpleVectorDB(
            'ragVectorDB', 
            partial(generate_embeddings_batch, model='text-embedding-v3', emb_client=ali_client)
        )
        self.client = client
    
    def db_prepare(self, doc_dir: str):
        documents = load_documents(doc_dir)
        preprocessed_chunks = split_into_clean_chunks(documents)
        self.v_db.batch_add_documents(preprocessed_chunks)
    
    def chat(self, 
            query: str,
            model: str = "qwen-long",
            max_tokens: int = 512,
            temperature: float = 1,
            top_p: float = 0.9,
            top_k: int = 50
    ):
        """
        Args:
            query (str): æé—®
            model (str): LLM default "qwen-long".
            max_tokens (int): ç”Ÿæˆå›ç­”çš„æœ€å¤štokensæ•°  Default is 512.
            temperature (float): Sampling temperature for response diversity. Default is 0.5.
            top_p (float): Probability mass for nucleus sampling. Default is 0.9.
            top_k (int): Number of highest probability tokens to consider. Default is 50.
        """
        relevant_chunks = self.v_db.search(query)
        prompt = construct_prompt(query, relevant_chunks)
        response = generate_response(
            prompt, 
            model=model, 
            client_in=self.client,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k
        )
        return response


def VDB_RAG_test():
    cur_p = os.path.dirname(__file__)
    dir_p = os.path.join(cur_p, "data")
    ali_client = OpenAI(api_key=ali_api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
    chat_box = VDB_RAG_Bot(
        collection_name='ragVectorDB', 
        client=ali_client,
        embedding_fn=partial(generate_embeddings_batch, model='text-embedding-v3', emb_client=ali_client)
    )
    chat_box.db_prepare(dir_p)

    test_f = os.path.join(cur_p, "data", 'val.json')
    with open(test_f, 'r') as file:
        validation_data = json.load(file)

    sample_query = validation_data['basic_factual_questions'][0]['question']  
    expected_answer = validation_data['basic_factual_questions'][0]['answer']  

    print(f"Sample Query: {sample_query}\n")
    print(f"Expected Answer: {expected_answer}\n")

    print("ğŸ” Running the Retrieval-Augmented Generation (RAG) pipeline...")
    print(f"ğŸ“¥ Query: {sample_query}\n")
    
    response = chat_box.chat(sample_query)
    print("ğŸ¤– AI Response:")
    print("-" * 50)
    print(response.strip())
    print("-" * 50)

    # Print the ground truth answer for comparison
    print("âœ… Ground Truth Answer:")
    print("-" * 50)
    print(expected_answer)
    print("-" * 50)
    response_embedding = generate_embeddings_batch([response])[0]
    ground_truth_embedding = generate_embeddings_batch([expected_answer])[0]
    similarity = cosine_similarity(response_embedding, ground_truth_embedding)
    
    print(f"âœ… similarity: {similarity:.5f}")

```
è¾“å‡ºå¦‚ä¸‹ï¼š
```
Sample Query: What is the mathematical representation of a qubit in superposition?
Expected Answer: |ÏˆâŸ© = Î±|0âŸ© + Î²|1âŸ©, where Î± and Î² are complex numbers satisfying |Î±|Â² + |Î²|Â² = 1, representing the probability amplitudes for measuring the qubit in state |0âŸ© or |1âŸ© respectively.

ğŸ” Running the Retrieval-Augmented Generation (RAG) pipeline...
ğŸ“¥ Query: What is the mathematical representation of a qubit in superposition?

model='qwen-long'
ğŸ¤– AI Response:
--------------------------------------------------
A qubit in superposition is mathematically represented as:

\[ \alpha|0\rangle + \beta|1\rangle \]

where \( \alpha \) and \( \beta \) are complex numbers called probability amplitudes, and \( |0\rangle \) and \( |1\rangle \) are the basis states of the qubit. The probabilities of measuring the qubit in state \( |0\rangle \) or \( |1\rangle \) are given by \( |\alpha|^2 \) and \( |\beta|^2 \), respectively, with the condition that \( |\alpha|^2 + |\beta|^2 = 1 \).
--------------------------------------------------
âœ… Ground Truth Answer:
--------------------------------------------------
|ÏˆâŸ© = Î±|0âŸ© + Î²|1âŸ©, where Î± and Î² are complex numbers satisfying |Î±|Â² + |Î²|Â² = 1, representing the probability amplitudes for measuring the qubit in state |0âŸ© or |1âŸ© respectively.
--------------------------------------------------
âœ… similarity: 0.91612
```